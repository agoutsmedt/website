---
title: "Scraping Working Papers from the European Central Bank Website"
author: "Aur√©lien Goutsmedt"
date: "2023-11-13"
slug: "scraping-ecb"
categories:
- Scraping
- R Tutorials
- Central Banks
tags:
- Scraping
- R
- Research Tools
- Quantitative Analysis
- Central Banks
subtitle: "Scraping Tutorials 2"
summary: "In this post, you will learn how to scrape various research documents from the [European Central Bank](https://www.ecb.europa.eu/home/html/index.en.html) website."
authors: []
lastmod: "`r Sys.Date()`"
featured: no
draft: no
lang: en
bibliography: bibliography.bib
link-citations: yes
color-links: yes
linkcolor: teal
urlcolor: blue
citecolor: red
endnote: no
toc: yes
number_sections: true
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

{{% toc %}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, fig.dim=c(14,10), fig.align = "center")
```

After a [first post](/post/scraping-bis) on scraping the [Bank of International Settlements database of speeches](https://www.bis.org/cbspeeches/index.htm?m=256), this post teaches how to scrape documents from the [European Central Bank](https://www.ecb.europa.eu/home/html/index.en.html) website. More accurately, we won't scrape the speeches of the ECB's board members, as they are already available in a `.csv` format [here](https://www.ecb.europa.eu/press/key/html/downloads.en.html). Rather, we will tackle research documents via the various series of working papers published by the ECB. 

In the previous post, we learnt the difference between static and dynamic webpages. Dynamic webpages involve interaction with your web browser: in this case, you need the [RSelenium](https://docs.ropensci.org/RSelenium/) [@R-rselenium] package.^[See the end of this tutorial for some tricks to "transform" complex dynamic webpages in simpler pages and not use *RSelenium*.] 

Here again, we will be confronted to dynamic webpages. However, these webpages raise novel issues. With the BIS database, the main issue was to go to the next page to scrape the metadata of the following speeches. We did it by [finding how the page number was integrated into the URL](/post/scraping-bis/#going-on-the-bis-website-and-launching-the-bot). With the ECB website, the problem will be trickier for two reasons: 

- First, all the documents are on the same page but as the page may be very long (depending on the type of documents), the page is not loaded entirely by your browser. You will thus learn to scroll down until all the documents are loaded. 
- Second, some information (like the abstract or the [JEL codes](https://www.aeaweb.org/econlit/jelCodes.php?view=jel) of a working paper) are hidden, and you have to click on a button to display them.

Before exploring that in details, let's load the needed packages. This time, we will use [rvest](https://rvest.tidyverse.org/) [@R-rvest] in complement of *RSelenium* and [polite](https://dmi3kno.github.io/polite/) [@R-polite]. At the end of this post, we will test a second method with *rvest* only and not *RSelenium* to scrape working papers (but don't spoil yourself!).

```{r needed-packages}
# pacman is a useful package to install missing packages and load them
if(! "pacman" %in% installed.packages()){
  install.packages("pacman", dependencies = TRUE)
}

pacman::p_load(tidyverse,
               RSelenium,
               rvest,
               polite,
               glue)
```

## A look at the ECB Website

Let's say we want to scrape the metadata of ECB's "occasional papers" at "https://www.ecb.europa.eu/pub/research/occasional-papers/html/index.en.html".

```{r show-op, fig.cap="ECB's occasional papers",  echo=FALSE}
knitr::include_graphics("occasional_papers.png")
```

```{r stoping-browser, echo=FALSE}
tryCatch({ # This is just to rerun the rmarkdown
  if(remDr$server$process$get_status() == "running") remDr$server$stop()
}, error = function(e){
  message("No server running")
}
)
```

```{r number-papers, echo=FALSE, results='hide'}
ecb_op_path <- "https://www.ecb.europa.eu/pub/research/occasional-papers/html/index.en.html"

remDr <- rsDriver(browser = "firefox",
                  port = 4444L,
                  chromever = NULL)
browser <- remDr[["client"]]
browser$navigate(ecb_op_path)
Sys.sleep(3)
browser$findElement("css", "a.cross.linkButton.linkButtonLarge.floatRight.highlight-extra-light")$clickElement() # Refusing cookies

nb_op <- browser$findElement("css selector", "dd:nth-child(2) .category")$getElementText()[[1]] %>% 
  str_extract(., "\\d+$")
```

We can see that there is `r nb_op` occasional papers on the ECB website. Let's try to scrape them. First, we use *polite* to "bow" in front of the ECB website. We specify who we are in the `user_agent` parameter, in order for the webmaster to contact us in case of problem. Indeed, we are trying here to follow some scraping ethical guidelines (find more developments on this issue [here](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)). The `bow()` function allows us to look at permissions for scraping.

```{r bowing-bis}
ecb_op_path <- "https://www.ecb.europa.eu/pub/research/occasional-papers/html/index.en.html"

session <- bow(ecb_op_path,
               user_agent = "polite R package - used for https://github.com/agoutsmedt/central_bank_database (aurelien.goutsmedt[at]uclouvain.be)")
session
```

By looking at the *robots.txt* file, we can see more in details which paths of the website are scrapable or not. Nothing about the publications path here!
 
```{r robotstxt}
cat(session$robotstxt$text)
```

Let's launch our bot.
```{r eval=FALSE}
remDr <- rsDriver(browser = "firefox",
                  port = 4444L,
                  chromever = NULL)
browser <- remDr[["client"]]
browser$navigate(ecb_op_path)
Sys.time(session$delay) # letting the page load
browser$findElement("css", "a.cross.linkButton.linkButtonLarge.floatRight.highlight-extra-light")$clickElement() # Refusing cookies
```

## Scrolling down and opening supplementary menus

We can try something quickly: we extract the list of the occasional papers number (the "No. XXX" information just above the title).

```{r extract-categories}
categories <- browser$findElements("css selector", ".category") %>% 
  map_chr(., ~.$getElementText()[[1]]) 

tail(categories)
```

We got something weird here. The last paper we got in the list is `r tail(categories, 1)`. But if we scroll all the way down to the page, we can see that the last paper should be "No. 1".

```{r show-last-op, fig.cap="ECB's occasional papers",  echo=FALSE}
knitr::include_graphics("last_occasional_papers.png")
```

Let's dig a bit into the `html` code of the webpage. In Firefox, you can go to the supplementary tools and open the "web development tools". You can see that there is something called `lazyload-container`.

```{r lazyload, fig.cap="Digging into ECB's website source code",  echo=FALSE}
knitr::include_graphics("lazyload.png")
```

As the number of occasional papers is quite big, for efficiency reasons, the website does not load the whole information. Consequently, if you want to scrape all the papers, you need to scroll down the page, and you need to scroll progressively to be sure that all the containers are loaded. The `executeScript()` function of *RSelenium* allows you to use some JavaScript code. Here, you don't want to scroll horizontally, but you want to scroll vertically: you thus need to define the number of pixels by which you want to scroll down (let's use 1600 to be sure that we will scroll down slowly). We will repeat this operation a certain number of time: as scrolling down by 1600 pixels is equivalent to scroll down by (a bit more than) five papers, we divide the number of papers by 5, to know the number of iterations of the scrolling down move.

```{r}
scroll_height <- 1600
scroll_iteration <- str_extract(categories[1], "\\d+") %>% 
  as.numeric()/5

  for(i in 1:ceiling(scroll_iteration)) {
    browser$executeScript(glue("window.scrollBy(0,{scroll_height});"))
    Sys.sleep(0.3)
  }
```

Let's check that now all the discussion papers are available: 

```{r}
categories <- browser$findElements("css selector", ".category") %>% 
  map_chr(., ~.$getElementText()[[1]]) 

length(categories)
```

That's all good. But we face a second issue: some important information are not visible yet. Indeed, by clicking on the "Details" button, you can access the abstract and the JEL codes of the paper. 

```{r details, fig.cap="The 'details' menu",  echo=FALSE}
knitr::include_graphics("details.png")
```

But as long as the *Details* menu is closed, the information is not accessible. So we need to find all the buttons that allow to click on the "Details" menu. But be careful, we want only the "Details" button, and not other types of button. As in the [former post]((/post/scraping-bis)), the [ScrapeMate](https://addons.mozilla.org/fr/firefox/addon/scrapemate/) Firefox add-on is essential to find the right *CSS selector*.

```{r scrapemate, fig.cap="Using ScrapeMate Beta",  echo=FALSE}
knitr::include_graphics("scrapemate.png")
```

We can now click on all the "Details" buttons, and only the "Details" buttons:

```{r}
  buttons <- browser$findElements("css selector", ".ecb-langSelector+ .accordion .header:nth-child(1) .title")
  for(i in seq_along(buttons)){
    buttons[[i]]$clickElement()
    Sys.sleep(0.4)
  }
```

Now, we have at our disposal all the information that is of interest to us. The remaining issue is that some information may be missing (or non-existent) for some papers (for instance, one paper has no author). So we extract the dates separately, and then we select the whole set of information for each paper (in the `papers_whole_information` object), and in this set, we extract in turn the title, the authors, the URL of the pdf, and the information in details.^[If there is no author, we will have a blank string.] 

```{r}
papers_whole_information <- browser$getPageSource()[[1]] %>% # we extract the whole info of each paper here (without the date)
  read_html() %>%
  html_elements("dd")

metadata <- tibble(
  category = categories,
  dates = browser$findElements("css selector", ".loaded > dt") %>% 
    map_chr(., ~.$getElementText()[[1]]),
  titles = papers_whole_information %>% 
    html_elements(".category+ .title a") %>% 
    html_text(),
  authors = papers_whole_information %>% 
    html_elements("ul") %>% 
    html_text2(),
  details = papers_whole_information %>% 
    html_elements(".ecb-langSelector+ .accordion .content-box:nth-child(2)") %>%
    html_text2(),
  url_pdf = papers_whole_information %>% 
    html_elements(".authors+ .ecb-langSelector .pdf") %>% 
    html_attr("href") 
)

head(metadata)
```

Two points of clarification here:

- the *CSS selector* must be selected carefully. For instance, a paper may have several ".title a", because it gathers some complementary documents. Paper 209 is an example. So you only want the title after the category. Hence the selector ".category+ .title a". That's the same logic for the "details" menu: you don't want what is in the Annex for instance. Also, you may have several pdf links, and you just want the pdf link below the authors, which is the pdf of the paper.
- the `authors` and `details` columns remain to be cleaned. Indeed, you can have several authors, and the `details` column gathers information about the abstract and JEL_codes, but also, in rare occasion, the "taxonomy" and the "network". That's why we use the `html_text2()` function of *rvest*: each type of information is separated by a linebreak ("\\n").

Let's clean the details. You first separate the lines and get something like that:

```{r}
details_cleaned <- metadata %>% 
  select(category, details) %>% 
  separate_longer_delim(details, "\n") %>% 
    filter(! details == "")

head(details_cleaned, 20)
```

What you need to do is to extract the category of information ("Abstract|JEL Code|Taxonomy|Network"), and the text below represents the content of this category of information. You can then pivot the data to fill each column for the paper. Of course, most columns for taxonomy and network will be empty. As you have several JEL codes for each paper, `pivot_wider()` creates list columns for each type of information. But you can "unnest" the columns with single values ("Abstract", "Taxonomy", and "Network")

```{r}
details_cleaned <- details_cleaned %>% 
  mutate(type = str_extract(details, "^Abstract$|^JEL Code$|^Taxonomy$|^Network$")) %>% 
  fill(type, .direction = "down") %>% # we replace NA values in the type column by the information above which is non NA
  filter(! str_detect(details, "^Abstract$|^JEL Code$|^Taxonomy$|^Network$")) %>% # we remove the title rows
  pivot_wider(names_from = type, values_from = details) %>% 
  unnest(cols = c("Abstract", "Taxonomy", "Network")) 

head(details_cleaned)
```

Let's "zoom" to the `JEL code` column:
```{r}
head(details_cleaned %>% select(category, `JEL Code`) %>%  unnest(`JEL Code`))
```


Finally, you just need to merge the details cleaned with the metadata:

```{r}
metadata_cleaned <- metadata %>% 
  select(-details) %>% 
  left_join(details_cleaned)

head(metadata_cleaned)
```

And you now have a nice table of metadata for ECB occasional papers! The [post on the BIS](/post/scraping-bis) explains then how to download the PDF, and how to run OCR if the text of the PDFs is not searchable. The code in this current post is also working for ECB's "discussion papers" and "working papers".

## Another Method?

You can use the method above to scrape the metadata for the almost 3000 thousands working papers of the ECB. But you can imagine that it takes a very long time to scroll down the page, and above all to open all the "Details" menu. *RSelenium* is wonderfully useful to interact with web browser, but sometimes, by digging into the source code of a webpage, you may find easier ways to extract relevant information. 

And this is possible for the ECB research pages. We already add a clue of it above in this post. Indeed, let's look again at the code related to the "lazyload-container". 

```{r lazyload-again, fig.cap="Digging into ECB's website code",  echo=FALSE}
knitr::include_graphics("lazyload.png")
```

Actually it tells us that the website is loading some other pages, ending with "papers-2023.include.en.html", "papers-2022.include.en.html", etc., until "papers-1999.include.en.html". Let's try the first one: "https://www.ecb.europa.eu/pub/research/working-papers/html/papers-2023.include.en.html"

```{r, fig.cap="A simpler ECB page",  echo=FALSE}
knitr::include_graphics("new_ecb_page.png")
```

So we can scrape quite easily the occasional-papers for 2023:

```{r other-ecb-method, eval = FALSE}

url <- "https://www.ecb.europa.eu/pub/research/occasional-papers/html/papers-2023.include.en.html"

html_info <- read_html(url) %>%
  html_elements(xpath = "/html/body/dd")

metadata <- tibble(
  category = html_info %>% 
    html_elements(".category") %>% 
    html_text(),
  dates = read_html(url) %>%
    html_elements(".date") %>%
    html_text(),
  titles = html_info %>% 
    html_elements(".category+ .title a") %>% 
    html_text(),
  authors = html_info %>% 
    html_elements("ul") %>% 
    html_text2(),
  details = html_info %>% 
    html_elements(".ecb-langSelector+ .accordion .content-box:nth-child(2)") %>%
    html_text2(),
  url_pdf = html_info %>% 
    html_elements(".pdf") %>% 
    html_attr("href") 
)

head(metadata)
```

## References {-}

