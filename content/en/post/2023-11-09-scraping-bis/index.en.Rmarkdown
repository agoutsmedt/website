---
title: "Scraping Central Bankers' Speeches from the BIS Website"
author: "Aur√©lien Goutsmedt"
date: "2023-11-09"
slug: "scraping-bis"
categories:
- Scraping
- R Tutorials
- Central Banks
tags:
- Scraping
- R
- Research Tools
- Quantitative Analysis
- Central Banks
subtitle: Scraping Tutorials 1
summary: "In this post, you will learn how to scrape speeches from the [Bank of International Settlements database](https://www.bis.org/cbspeeches/index.htm?m=256) which gathers most of central bankers' speeches in English."
authors: []
lastmod: "`r Sys.Date()`"
featured: no
draft: no
lang: en
bibliography: bibliography.bib
suppress-bibliography: no
link-citations: yes
color-links: yes
linkcolor: teal
urlcolor: blue
citecolor: red
endnote: no
toc: yes
number_sections: true
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

{{% toc %}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, fig.dim=c(14,10), fig.align = "center")
```



## Scraping data

In this post, you will learn how to scrape speeches from the [Bank of International Settlements database of speeches](https://www.bis.org/cbspeeches/index.htm?m=256), which gathers most of central bankers' speeches in English. We will do it programming in R. The next [post](/post/scraping-ecb) focuses on scraping documents from the [European Central Bank](https://www.ecb.europa.eu/home/html/index.en.html) website.

Before going along the scraping, let's start by a general introduction about what web scraping is.

### What is web scraping?

Web scraping is the process of extracting data from websites. It involves programmatically accessing a website's content, extracting the data you want, and then saving it for further use. It involves going to a specific URL, retrieving the html content and extract the data you want from this content.

In general, you need to use web scraping when there is no [API](https://en.wikipedia.org/wiki/API) (Application Programming Interface) for the website data you want to access. An API is a structured and authorized way to access data provided by a website, from a system of queries (for instance, [scopus](https://www.scopus.com/) provides an API to [retrieve its bibliometric  data](../extracting-biblio-data-1/)). 

Scraping data on a website implies various ethical and legal issues (for which I am not a specialist at all!). First, many websites have terms of service agreements that explicitly prohibit web scraping. Engaging in scraping without the website owner's consent violates these terms and conditions. For instance, [Linkedin](https://www.linkedin.com/) prohibits scraping on its website. However, the "legality" of scraping Linkedin [remains a complex matter](https://www.forbes.com/sites/zacharysmith/2022/04/18/scraping-data-from-linkedin-profiles-is-legal-appeals-court-rules/). Before engaging in scraping, you need to know what are the terms and conditions, and know what are the risks if you violate them.

A second issue has to do with overloading servers. Web scraping can put a significant load on a website's servers, potentially leading to decreased performance or downtime. Scraping without proper rate limiting and respect for a website's capacity can be seen as unethical and disruptive.

Third, if you may be allowed to scrape a website, it does not mean that you can copy and distribute the materials collected. You should be aware of copyright laws and intellectual property rights when diffusing scraped content.

### Some useful resources

When talking about web scraping with R, three R packages stand out (but we will only use two of them here):

- [rvest](https://rvest.tidyverse.org/) [@R-rvest]: a package in the [tidyverse](https://www.tidyverse.org/) spirit that helps to extract data from web pages. It works better with static web page (see below). 
- [RSelenium](https://docs.ropensci.org/RSelenium/) [@R-rselenium]: this is an R binding for [Selenium](https://www.selenium.dev/) 2.0 Remote WebDriver, which allows you to automate the use of a browser. In other words, it enables you to write a script to open a web browser and interact with a website (yes, this is where scraping starts to get fun).
- [polite](https://dmi3kno.github.io/polite/) [@R-polite]: this package allows you to look at the [robots.txt](https://en.wikipedia.org/wiki/Robots.txt) of a website to know what are the scraping rules for this website, and to avoid hammering the site with too many requests.

Depending on the website you want to scrap, using [rvest](https://rvest.tidyverse.org/) may be sufficient (as it is the case, for instance, with [Wikipedia](https://en.wikipedia.org/wiki/Main_Page)). For more complicated website, you may need [RSelenium](https://docs.ropensci.org/RSelenium/), in combination with [rvest](https://rvest.tidyverse.org/) or alone. Indeed, it depends if the web pages you want to scrape are static or dynamic web pages. As explained by [Ivan Milanes' useful blog post](https://ivanmillanes.netlify.app/post/2020-06-30-webscraping-with-rselenium-and-rvest/):

- [Static Web Page](https://www.pcmag.com/encyclopedia/term/static-web-page): A Web page (HTML page) that contains the same information for all users. Although it may be periodically updated from time to time, it does not change with each user retrieval.
- [Dynamic Web Page](https://www.pcmag.com/encyclopedia/term/dynamic-web-page): A Web page that provides custom content for the user based on the results of a search or some other request. Also known as "dynamic HTML" or "dynamic content", the "dynamic" term is used when referring to interactive Web pages created for each user.

The web page where central bankers' speeches are gathered is a dynamic web page and so we will use *RSelenium* to scrape the speeches.^[I am not a specialist of html structures, but I was unable to extract the data using *rvest* only. I thus used *RSelenium*, and *RSelenium* only, even if most of the time you combine both packages.]

To complement this tutorial, you will find two other useful tutorials for scraping using *rvest* and *RSelenium* [here](https://www.rselenium-teaching.etiennebacher.com/#/title-slide) and [here](https://resulumit.com/teaching/scrp_workshop). There is also a [whole chapter dedicated to web scraping](https://r4ds.hadley.nz/webscraping) in the recent second edition of *R for Data Science* [@wickham2023].

## Scraping the BIS speeches

If we are interested in what central bankers are saying, we may want to study their speeches. Fortunately, most of them are stored on the BIS website [there]("https://www.bis.org/cbspeeches/index.htm"). 

```{r bis-website, fig.cap="The BIS list of speeches", echo=FALSE}
knitr::include_graphics("bis.png")
```

What we want is 

- to extract the metadata of these speeches: the date of the speech, the title, its description and the speaker. 
- to access to the whole text of the speech, that is stored in a pdf on the BIS website
- to extract the text from this PDF
- to be sure that the text is readable, which will be the case if the PDF is searchable (i.e. if characters are recognized). 

This is all these things we will learn to do in this tutorial.

### Loading the packages and starting your automated browser

We first load the necessary packages. [RSelenium](https://docs.ropensci.org/RSelenium/) [@R-rselenium] and [polite](https://dmi3kno.github.io/polite/) are the central packages here. I like the [here](https://here.r-lib.org/) package to manipulate paths with R but it is not essential here... [glue](https://glue.tidyverse.org/) is used to create strings more easily, by embedding R expressions in it. [pdftools](https://docs.ropensci.org/pdftools/) will be used to extract text from the speeches' PDFs and [tesseract](https://docs.ropensci.org/tesseract/) for running Optical Character Recognition (OCR) on PDFs that don't have a good OCR.

```{r needed-packages}
# pacman is a useful package to install missing packages and load them
if(! "pacman" %in% installed.packages()){
  install.packages("pacman", dependencies = TRUE)
}

pacman::p_load(tidyverse,
               RSelenium,
               polite,
               glue,
               here,
               pdftools,
               tesseract)

bis_data_path <- here(path.expand("~"), "data", "central_bank_database", "BIS") # Where data will go on my computer
```

### Going on the BIS website and launching the bot

We use *polite* to "bow" in front of the BIS website. It allows us to look at permissions for scraping.

```{r bowing-bis}
bis_website_path <- "https://www.bis.org/"

session <- bow(bis_website_path)
session
```

By looking at the *robots.txt* file, we can see more in details which paths of the website are scrapable or not. 
 
```{r robotstxt}
cat(session$robotstxt$text)
```

The list of speeches is at "https://www.bis.org/cbspeeches/index.htm". This path is not among the "disallow" list, meaning that we can scrape this part of the website.

We want to make scraping easier, so we will display the maximum number of items per page, which is 25 here. We don't want to scrape the whole list of speeches, so we will set a starting date: the ninth of October, 2023. By navigating towards the second page of the resulting list, you can now have an idea of what the URL looks like.

```{r bis-filtered, fig.cap="Filtering speeches on the BIS website",  echo=FALSE}
knitr::include_graphics("bis_filtered.png")
```

Thus, you can prepare a string with the URL to scrape, in which you can change the parameters. 

```{r scraping_path}
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page={page}&cbspeeches_page_length=25"))
```

Here, I could have used some simpler programming by using the `paste0()` function for instance, rather than using `expr()`. But I want to be able to change the parameters later, notably for the page number. For now, the path looks like this:

```{r example-expr}
scraping_path
```

But then I can enter the parameters I want for the date (here, the ninth of October, 2023).

```{r entering-date}
day <- "09"
month <- "10"
year <- 2023
```

And I can evaluate the expression with these parameters, also selecting a certain page. This will be useful later in a loop to navigate from one page to the next.

```{r example-eval}
eval(scraping_path, envir = list(page = 3))
```

You can check that this URL leads you to the third page of the list of speeches starting the ninth of October, 2023 (with 25 results per page).

Once you have the structure of the URL, it means that you can change the parameters as you want to create the list of speeches you want. Of course, you can go on the BIS website and filter by the end date or the "Institutions" or the "Country" to see what the URL would look like.

Now we can start the funny part. Let's launch our automated browser with *RSelenium*:

```{r launch-browser, results="hide"}
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
                   chromever = NULL,
                   port = 4444L) 
remote_driver <- driver[["client"]]
```

```{r bot-picture, fig.cap="Automatizing your web browser",  echo=FALSE}
knitr::include_graphics("featured.png")
```

### Extracting the data page by page

What we want to do is to retrieve the data about each speech (the date, the title, its description, the speaker, and also its corresponding URL) page by page. This is something that we can do with a `for` loop. But we first need to know the number of pages. You can normally see it at the bottom right of the web page. 

There could be different way to extract this information. Here, we will use the "CSS selector". Using the CSS selector has two advantages:

- It has in general a simpler content than an XPATH
- It can be easily retrieved thanks to a Firefox add-on (which exists also for Chrome): [ScrapeMate](https://addons.mozilla.org/fr/firefox/addon/scrapemate/)

Once you have set up the add-on, you can go to the URL we are working on and find the "CSS selector" for the number of pages:

```{r scrape-mate, fig.cap="Using scrapemate Firefox addon",  echo=FALSE}
knitr::include_graphics("scrapmate.png")
```

You can use this CSS selector to extract the text thanks to *RSelenium* and after a short cleaning, you have the total number of pages:

```{r nb-pages}
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page

nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
  pluck(1) %>% 
  str_remove_all("Page 1 of ") %>%
  as.integer()

nb_pages
```

Ok. Now we can run the loop to retrieve all the data for the `r nb_pages` pages. Here, we will conform to the delay set by the polite package: we will wait `r session$delay` seconds on each page, not to hammer the website with too many requests. As we need the web page to load properly to retrieve the data, it is also a way to take the time of loading the page to avoid missing data if your internet connection is too slow.

Here, you just have to use *scrapemate* to find the appropriate CSS selector for each information and extract the corresponding text. You can also see that by clicking on the title of the speech, you are navigating towards the dedicated web page of the corresponding speech. It means that to the title is associated a URL link, a "href" in html language. So you need to extract the "href" attribute associated to the title thanks to *RSelenium*.

```{r extract-metadata}
metadata <- tibble(date = c(),
                   title = c(),
                   description = c(),
                   speaker = c(),
                   url = c())
for(page in 1:nb_pages){
  remote_driver$navigate(eval(scraping_path))
  nod <- nod(session, eval(scraping_path)) # introducing to the new page
  Sys.sleep(session$delay) # using the delay time set by polite

dates <- remote_driver$findElements("css selector", ".item_date")
titles <- remote_driver$findElements("css selector", ".dark")
descriptions <- remote_driver$findElements("css selector", "p p")
speakers <- remote_driver$findElements("css selector", "span+ p")
urls <- remote_driver$findElements("css selector", ".dark")

metadata <- metadata %>% 
  bind_rows(tibble(date = map_chr(dates, ~.$getElementText()[[1]]),
                   title = map_chr(titles, ~.$getElementText()[[1]]),
                   description = map_chr(descriptions, ~.$getElementText()[[1]]),
                   speaker = map_chr(speakers, ~.$getElementText()[[1]]),
                   url = map_chr(urls, ~.$getElementAttribute("href")[[1]])))
}

head(metadata)
```

The `findElements()` function of *RSelenium* finds all the corresponding elements (here 25) and stores them in a list. The `map_chr()` allows you to get the text of each element one by one and stores the result in a vector. 

If we want to be sure that we have retrieved the data of all the speeches, we can compare the length of our data frame, with the number of items that was indicated just above the first speech.

```{r check-items}
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
  pluck(1) %>%
  str_remove_all("[\\sitems]") %>% 
  as.integer()
if(nb_items == nrow(metadata)){
  cat("You have extracted the metadata of all speeches")
} else{
  cat("There is a problem. You have probably missed some speeches.")
}
```

### Downloading the PDF

Now we have the metadata of the speeches. But one thing we are interested in at the end is to know what the speeches are about. Most of the speeches are stored in a pdf. To retrieve these PDFs, you just need to use the URL of the speech, and replace the end of the URL ".htm" by ".pdf". From the metadata list, we can have the whole list of the PDFs we want to download.

```{r list-pdf}
pdf_to_download <- metadata %>% 
    mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
           pdf_name = str_c(pdf_name, ".pdf")) %>% 
    pull(pdf_name) 

pdf_to_download
```

We build a function to download these PDF using *polite*. It would avoid notably to overload the website.

```{r polite-download}
polite_download <- function(domain, url, ...){
  bow(domain) %>% 
    nod(glue(url)) %>% 
    rip(...)  
}
```

There is a possibility that some PDFs are missing (that is not the case here, but it happens with the whole dataset of BIS speeches). The problem is that if a PDF is missing, you will get an error and it will stop the process of downloading PDFs So you need to use the `tryCatch()` function to take into account this error but continue the downloading process.

```{r download-pdf, eval=FALSE}
tryCatch(
   {
     walk(pdf_to_download, ~polite_download(domain = bis_website_path,
                                            url = glue("review/{.}"),
                                            path = here(bis_data_path, "raw_data", "pdf"),))
   },
   error = function(e) {
     print(glue("No pdf exist"))
   }
  )
```

Here, the pdf has been stored in a specific repository on my computer, indicated by the parameter `path`.

## Extracting speeches text

Now that we have collected the PDFs, we want to extract the text within these PDFs. We will use the package [pdftools](https://docs.ropensci.org/pdftools/) for that. Here is a function that take the list of PDFs extracted from the metadata, extract the text, and stores it in a table with the name of the file and the corresponding page of the text (indeed, `pdf_text()` extract the text page by page). Here, we handle error in case we did not find the PDF: we don't want the extraction of text to stop, but we want to remember that a PDF is missing.

```{r extracting-function}
extracting_text <- function(file, path) {
  tryCatch(
    {  
  message(glue("Extracting text from {file}"))
  text_data <- pdf_text(here(path, file)) %>%
    as_tibble() %>% 
    mutate(page = 1:n(),
           file = file) %>% 
    rename(text = value)
    },
  error = function(e) {
    print(glue("No pdf exist"))
    text_data <- tibble(text = NA,
                        page = 1,
                        file = file)
  }
  )
}
```

We can run the function for each speech we have in our dataset. 

```{r extract-text}
text_data <- map(pdf_to_download, ~extracting_text(., here(bis_data_path, "raw_data", "pdf"))) %>% 
  bind_rows()

head(text_data)
```

### Managing OCR problems

Sometimes, PDFs have problems of OCR. The PDFs are no searchable and so the text cannot be extracted properly (or rather, it is unreadable). It happens also with recent speeches. Therefore, we need to find the PDFs that may encounter such problems. We count the number of words in each page, and retrieve the PDFs for which the number of words is low in each page. 

```{r ocr-problem}
pdf_without_ocr <- text_data %>% 
  mutate(test_ocr = str_count(text, "[a-z]+")) %>% # Count the number of words
  mutate(non_ocr_document = all(test_ocr < 250), .by = file) %>% 
  filter(non_ocr_document == TRUE) %>% 
  distinct(file) %>% 
  pull()

pdf_without_ocr
```

Setting the threshold at 250 words is a pure rule of thumb: it allows having only true positive, but it does not prevent for having false negative (missing documents with OCR problem). It should be possible to find a better method to assess which PDF have no OCR. 

Here is an example of a problematic PDF:

```{r look-ocr-problem}
text_data %>% 
  filter(file %in% pdf_without_ocr[1])
```

It seems ok when you look at it, but going to the web page allows us to see that there are some problem of selection in the text and so that the OCR is not adequate.

```{r show-ocr-problem, fig.cap="PDF without adequate OCR",  echo=FALSE}
knitr::include_graphics("problem_ocr.png")
```

We will now use the [tesseract](https://docs.ropensci.org/tesseract/) package, that runs OCR. It first converts each page of the PDF into a picture (a PNG), then proceeds to OCR and extracts the text.

```{r tesseract, eval=FALSE}
new_text_ocr <- vector(mode = "list", length = length(pdf_without_ocr))

for(i in 1:length(pdf_without_ocr)){
  text <- tesseract::ocr(here(bis_data_path, "raw_data", "pdf", pdf_without_ocr[i]), engine = tesseract::tesseract("eng"))
  text <- tibble(text) %>% 
    mutate(text = str_remove(text, ".+(=?\\\n)"),
           page = 1:n(),
           file = str_remove(pdf_without_ocr[i], "\\.pdf"))
  new_text_ocr[[i]] <- text
}

new_text_ocr <- bind_rows(new_text_ocr)
```

```{r tesseract_hidden, eval=TRUE, echo=FALSE}
for(i in 1){
  text <- tesseract::ocr(here(bis_data_path, "raw_data", "pdf", pdf_without_ocr[i]), engine = tesseract::tesseract("eng"))
  text <- tibble(text) %>% 
    mutate(text = str_remove(text, ".+(=?\\\n)"),
           page = 1:n(),
           file = str_remove(pdf_without_ocr[i], "\\.pdf"))
  new_text_ocr <- text
}
```

You can see that the text is now readable:

```{r show-correction}
new_text_ocr
```

Now you just have to integrate the now recognized text with the rest of the data. A last small step is to remove all the PNGs you have created by using *tesseract*.

```{r removing-pictures, results="hide"}
# We remove all the .png we have created
png_to_remove <- list.files()[str_which(list.files(), "_\\d+\\.png")]
file.remove(png_to_remove)
```

And that's it for the tutorial. You now have a data frame with the metadata of the speeches, and a data frame with the text, page by page for each speech.

If you want to scrap BIS speeches on a regular basis without scraping again what you have already collected, you may have a look at this script [here](https://github.com/agoutsmedt/central_bank_database/blob/master/scraping_scripts/scraping_bis.R), which is part of a [project to collect central banks' documents](https://github.com/agoutsmedt/central_bank_database/tree/master). You will also find in it some function to clean BIS data.

## References {-}

