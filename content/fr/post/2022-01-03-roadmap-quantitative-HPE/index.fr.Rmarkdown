---
title: "Petit guide pour apprendre R à destination des historiens de l'économie"
author: Aurélien Goutsmedt
date: '2022-01-03'
slug: ''
categories:
  - Bibliometrics
  - R Package
  - Network Analysis
tags:
  - Bibliometrics
  - Network Analysis
  - R
  - R Package
  - Topic-Modelling
  - Research Tools
  - Research Methods
subtitle: ''
summary: "Lorsque la tentation est grande de s'essayer aux méthodes quantitatives, la première question qui vient à l'esprit est probablement ≪ mais comment faire, et quels outils dois-je apprendre à utiliser ? ≫. Je donne ici quelques arguments pour vous engager dans l'apprentissage de R et présente ensuite différents tutoriels et packages R utiles aux historiens de l'économie."
bibliography: bibliography.bib
#csl:  chicago-author-date.csl
suppress-bibliography: false
link-citations: true
color-links: true # See https://ctan.org/pkg/xcolor for colors
linkcolor: teal
urlcolor: blue
citecolor: red
endnote: false
toc: true
authors: []
lastmod: '`r Sys.time()`'
featured: no
draft: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

```{js zoom-jquery, echo = FALSE}
 $(document).ready(function() {
    $('body').prepend('<div class=\"zoomDiv\"><img src=\"\" class=\"zoomImg\"></div>');
    // onClick function for all plots (img's)
    $('img:not(.zoomImg)').click(function() {
      $('.zoomImg').attr('src', $(this).attr('src')).css({width: '100%'});
      $('.zoomDiv').css({opacity: '1', width: 'auto', border: '1px solid white', borderRadius: '5px', position: 'fixed', top: '50%', left: '50%', marginRight: '-50%', transform: 'translate(-50%, -50%)', boxShadow: '0px 0px 50px #888888', zIndex: '50', overflow: 'auto', maxHeight: '100%'});
    });
    // onClick function for zoomImg
    $('img.zoomImg').click(function() {
      $('.zoomDiv').css({opacity: '0', width: '0%'}); 
    });
  });
```

{{% toc %}}

## Pourquoi apprendre R ?

Les débats sur l'utilisation de méthodes quantitatives dans l'étude de l'histoire de l'économie ont fleuri ces dernières années - voir par exemple le numéro spécial dans le *Journal of Economic Methodology* [@edwards2018]- et de nombreuses contributions récentes ont utilisé la bibliométrie, l'analyse de réseau ou la modélisation des sujets. ^[Pour les lecteurs francophones, vous trouverez une introduction aux données et méthodes quantitatives appliquées à l'histoire de l'économie, ainsi que plusieurs exemples d'utilisation de ces méthodes [ici](../../courses/mehpere/02-2021-quantitative_analysis/)]. Lorsque l'envie vous prend de vous essayer aux méthodes quantitatives (comme cela a été le cas pour moi il y a quelques années), la première question que vous vous posez est sans doute "mais comment faire, et quels outils dois-je apprendre à utiliser ?".

Il existe de nombreux logiciels (certains gratuits, d'autres payants) qui vous permettront d'utiliser la bibliométrie, l'analyse de réseaux ou la modélisation de sujets. En fonction de ce que vous voulez faire, vous devrez apprendre chaque logiciel correspondant--après vous être assuré que l'outil est approprié pour accomplir la tâche que vous avez à l'esprit. C'est ce que j'ai fait lorsque j'ai voulu utiliser l'analyse de réseau pour l'un de mes [articles](../../publication/isom/) : J'ai appris à utiliser le très bon [GEPHI](https://gephi.org/). Cependant, Gephi est très utile pour manipuler des réseaux, mais à condition que vous ayez déjà construit votre réseau. Et si vous souhaitez utiliser d'autres types d'analyse quantitative sur vos données (par exemple l'analyse de texte et la modélisation thématique), vous devrez trouver un autre logiciel. C'est pourquoi, après ce premier projet de recherche intégrant des méthodes quantitatives, j'ai pris une autre direction : J'ai appris à programmer en R.^[Je ne discuterai pas ici de la question de savoir si R ou Python convient mieux aux enquêtes quantitatives. Mon choix de R est totalement dépendant du chemin parcouru : Je travaillais à l'époque sur un projet collectif dont certains membres programmaient en R. Il était plus facile pour moi de suivre leur chemin et de leur demander de l'aide].

Il est évident que l'apprentissage de R a un coût d'entrée plus élevé que l'apprentissage de GEPHI - 2 ou 3 heures devraient suffire pour apprendre les bases de ce dernier. Mais il offre de nombreux avantages et, à moyen et long terme, de grandes économies d'échelle :

- Il vous permettra de manipuler vos données plus facilement et plus rapidement qu'en utilisant Excel (que ce soit de manière programmatique ou manuelle). Vous pourrez faire des choses que vous ne pouvez pas faire avec Excel. Et cela devient de plus en plus vrai au fur et à mesure que votre ensemble de données s'accroît.
- Vous pourrez utiliser différentes méthodes quantitatives (bibliométrie, analyse de réseau, traitement du langage naturel, économétrie, apprentissage automatique, *etc.*) et construire des visualisations en apprenant un seul et unique langage (grâce aux nombreux paquets implémentés dans R).
- L'écriture de "scripts" en R vous permet de réexécuter les mêmes tâches sans effort supplémentaire à chaque fois que vos données changent ou que vous souhaitez modifier un paramètre de votre analyse. En outre, ces scripts constituent une ressource que vous pourrez réutiliser dans d'autres projets.
- En dehors des méthodes quantitatives à proprement parler, l'apprentissage de R vous permettra de faire plusieurs autres choses :

  - rédiger efficacement livres, articles, rapports et diapositives grâce à [Rmarkdown](https://rmarkdown.rstudio.com/) [@R-rmarkdown] ;
  - publier en ligne vos analyses et construire des tableaux de bord et des applications^[Ce site est notamment réalisé grâce au package R [blogdown](https://pkgs.rstudio.com/blogdown/) [@R-blogdown]].

Si vous êtes convaincu que la programmation en R pourrait vous être utile en tant qu'historien de l'économie, vous avez maintenant besoin de ressources pour apprendre à coder. Voici une liste de packages et de tutoriels utiles que j'aurais aimé avoir lorsque j'ai commencé à apprendre à programmer en R et que j'ai beaucoup utilisés depuis pour améliorer mes compétences.


## Première étape : les bases

La toute première étape consiste évidemment à télécharger et à installer R et Rstudio sur votre ordinateur, ainsi qu'à comprendre les bases du langage R et la manière d'utiliser Rstudio. Le [premier chapitre](https://moderndive.netlify.app/1-getting-started.html) de @mcconville2021 ou cette [leçon](https://rladiessydney.org/courses/ryouwithme/01-basicbasics-0/) de [R-ladies Sydney](https://rladiessydney.org/) sont de bons points de départ.


Une fois que vous êtes un peu familiarisé avec ce qu'est R et le fonctionnement de Rstudio, je pense que la priorité est de vous familiariser avec la collection de paquets [tydiverse](https://www.tidyverse.org/) [@R-tidyverse ; @tidyverse2019]. Un guide très fiable est évidemment l'ouvrage de Wickham et Gromelund [-@wickham2017] [R for Data Science](https://r4ds.had.co.nz/index.html). Dans ce livre, vous apprendrez à importer des données, à les transformer, à les visualiser et à leur appliquer des modèles statistiques^[Un autre outil majeur lorsque vous traitez de très grands ensembles de données est le package [data.table](https://rdatatable.gitlab.io/data.table/) [@R-datatable], qui augmente considérablement la vitesse de programmation. Vous pouvez trouver de nombreux tutoriels sur les fonctionnalités de *data.table* sur le site web du package]. 

{{% callout note %}} Un bon moyen d'apprendre de manière active et interactive est d'essayer quelques lignes de code sur vos propres données. C'est quelque chose qui m'a aidé à apprendre rapidement (et à apprendre avec enthousiasme). Je me suis essayé au traitement et à la visualisation de données sur la base de données prosopographique que je construisais pour le [projet sur la Banque d'Angleterre](../../project/excavating-academia-policy/) collectif  auquel je participais. {{% /callout %}}

Un autre détour utile lorsque vous avez progressivement appris les bases de R, est de comprendre comment organiser votre flux de travail avec R et Rstudio (comment organiser vos fichiers ? pourquoi et comment créer un  "projet RStudio" ?) : vous pouvez trouver des informations utiles dans le [deuxième chapitre](https://stat545.com/r-basics.html) de @bryan2017. Je ne veux pas charger la barque trop tôt, mais lorsque vous aurez commencé votre premier projet, il pourrait être utile d'apprendre les bases du contrôle de version et de [git](https://git-scm.com/). C'est quelque chose qui sera indispensable lorsque vous travaillerez collectivement sur un projet dans Rstudio. Parce que l'utilisation de git n'est pas très intuitive et que vous vous arracherez probablement les cheveux au début, il vaut mieux s'habituer à utiliser git dès le début, même (et surtout ?) lorsque vous travaillez seul. Vous serez confronté à des cas simples et vous aurez alors moins peur d'utiliser le versioning git dans des projets collectifs. @bryan2019 propose une [documentation riche](https://happygitwithr.com/) pour apprendre à utiliser git et [github](https://github.com/)--ce dernier étant fondamental pour partager son code et donner des explications sur les projets sur lesquels on travaille.^[Voir par exemple la [page github](https://github.com/Alex7722/macro_AA) d'un projet sur lequel je travaille avec [Alexandre Truc](https://sites.google.com/view/alexandre-truc/home-and-contact).]

R offre de formidables outils pour construire une grande variété de visualisations. Le package [ggplot2](https://ggplot2.tidyverse.org/) [@R-ggplot2 ; @ggplot22016] est évidemment le premier de ces outils, et vous pouvez trouver une documentation dense [ici](https://ggplot2-book.org/). Le livre de Kieran Healy [-@healy2018] est également une bonne lecture pour apprendre *ggplot2* tout en réfléchissant aux meilleures pratiques pour construire des visualisations de données. Un des avantages de ggplot2 est qu'il est accompagné de nombreuses extensions (nouveaux types de visualisations, outils pour personnaliser de différentes manières les visualisations produites avec *ggplot2*, nouveaux thèmes et palettes de couleurs, animation, *etc*) listées [ici](https://exts.ggplot2.tidyverse.org/gallery/). Je reconnais que l'apprentissage de cette "grammaire graphique" est loin d'être facile au début, car vous devrez connaître de nombreuses fonctions et prendre en compte de nombreuses caractéristiques lors de la production de visualisations quantitatives. L'extension [esquisse](https://dreamrs.github.io/esquisse/articles/get-started.html) [@R-esquisse] pourrait vous permettre de réduire le coût d'entrée et de vous familiariser progressivement avec les fonctions de *ggplot2* : il s'agit d'un complément de Rstudio qui vous permet de construire interactivement vos visualisations. 

Si vous souhaitez produire des visualisations dynamiques et interactives, le paquet [plotly](https://plotly.com/r/) [@R-plotly ; @plotly2020] offre de nombreuses possibilités. L'avantage de *plotly* est que, pour certaines fonctionnalités de base, vous pouvez convertir les visualisations produites avec *ggplot2* en visualisations interactives avec une seule ligne de code. Mais *plotly* pour R permet également des personnalisations plus complexes qui nécessitent d'étudier plus en profondeur les différentes fonctions du package (voir une documentation détaillée [ici](https://plotly-r.com/)).

Voilà pour les bases : il est évident que vous ne maîtriserez pas tous les packages ci-dessus en un jour, et vous les découvrirez progressivement en travaillant sur votre propre projet de recherche. Cependant, vous aurez probablement besoin de la plupart d'entre eux dans tous les projets quantitatifs que vous entreprendrez, que vous souhaitiez effectuer une simple analyse statistique sur une base de données prosopographique d'économistes formés au MIT, une comparaison des mots utilisés par Adam Smith et David Ricardo, ou une analyse bibliométrique des publications sur la discrimination dans les revues académiques économiques. Les outils suivants que je vais vous présenter visent des usages plus spécifiques. 

## Apprendre la bibliométrie et l'analyse de réseau

La bibliométrie est utile aux historiens de l'économie pour comprendre comment la discipline a évolué, quelles sont les influences majeures pour des sous-domaines spécifiques, comment différentes communautés ont structuré la discipline à différentes périodes, *etc*^[Pour des exemples d'utilisation de la bibliométrie en histoire de l'économie, voir @claveau2016 ou @herfeld2019]. L'analyse de réseaux vient en complément de la bibliométrie pour produire des réseaux de couplage bibliographique, de co-citation, de mots-clés ou de co-auteurs. 

Un premier package, [bibliometrix](https://www.bibliometrix.org/), offre différentes fonctions allant de l'extraction de données bibliométriques à partir de différentes plateformes (comme Web of Science ou Scopus), au calcul de statistiques et à la construction de réseaux. Je ne l'ai jamais utilisé de manière intensive, pour deux raisons : 

1. Je travaillais sur de grands ensembles de données et *bibliometrix* semblait un peu lent pour cela ;
2. J'ai trouvé les fonctions un peu comme des boîtes noires, et pas vraiment compatibles avec l'esprit *tidyverse*. Cependant, il semble très riche et je suis sûr qu'il vaut la peine d'être exploré.

Une fois que vous avez vos données - ce qui est loin d'être une étape facile^[La collecte de données bibliométriques n'est pas une tâche facile, car elle dépend de votre accès aux différentes bases de données et aux types d'informations que vous recherchez. De plus, il y a souvent beaucoup de nettoyage à faire sur les données que vous avez pu télécharger. J'essaierai de faire un billet de blog clarifiant ces questions bientôt.]---J'ai développé, avec l'aide d'[Alexandre Truc](https://sites.google.com/view/alexandre-truc/home-and-contact) et de [François Claveau](https://www.epistemopratique.org/en/), un *package* pour rendre la création de très grands réseaux de données bibliographiques facile et rapide : [biblionetwork](https://agoutsmedt.github.io/biblionetwork/) [@goutsmedt_2023_7677369]. Ce dont vous avez besoin en entrée est un corpus et les références citées par ce corpus, et vous serez en mesure de créer facilement une liste de liens entre articles, références, auteurs, institutions, *etc.*, avec différentes mesures de poids pour ces liens. 

Une fois que vous avez vos arêtes/liens, vous pouvez créer un réseau à l'aide du package [igraph](https://igraph.org/r/) [@igraph2006]. *igraph* est un *package* très riche qui comporte de nombreuses fonctions permettant de calculer des statistiques sur les réseaux (comme les mesures de centralité), de trouver des grappes et d'organiser les nœuds dans l'espace. Cependant, la manipulation des réseaux avec *igraph* n'est pas toujours très intuitive. Heureusement, le package [tidygraph](https://tidygraph.data-imaginist.com/) [@R-tidygraph] vous permet de transformer les réseaux en données "ordonnées", dans l'esprit *tidyverse*, et de manipuler les réseaux en conséquence. De même, le paquet [ggraph](https://ggraph.data-imaginist.com/) [@R-ggraph], qui est une extension de *ggplot2*, permet de produire de belles visualisations de réseaux.

```{r, echo = FALSE, fig.align='center', fig.cap="Un exemple de réseau tiré du projet Mapping Macroeconomics"}
knitr::include_graphics('example-graph.png')
```

Alexandre Truc et moi-même développons actuellement un autre *package*, [networkflow](https://agoutsmedt.github.io/networkflow/), pour faciliter la manipulation des réseaux, grâce à *tidygraph* et *ggraph*. L'objectif est de construire des routines pour baliser et accélérer l'analyse des réseaux, depuis leur création jusqu'à leur visualisation, en passant par le clustering et la spatialisation. Dans un futur proche, le *package* intégrera des outils pour l'analyse dynamique des réseaux (en comparant l'évolution de différents réseaux dans le temps). 

## Essayez l'exploration de texte et la modélisation de sujets

Le text mining est une approche essentielle pour analyser de grands corpus et peut donc apporter des informations utiles aux historiens de l'économie. 

La première étape consiste à transformer le texte de vos documents en données lisibles dans R. Cela implique tout d'abord que le texte de vos documents soit reconnaissable. Pour vérifier cela, vous pouvez simplement copier le texte d'une page aléatoire de votre corpus et le coller dans un éditeur de texte. Si le texte n'est pas reconnaissable, ou si la qualité n'est pas bonne, vous devriez lancer [Optical Character Recognition](https://en.wikipedia.org/wiki/Optical_character_recognition). Dans mes anciens projets, j'ai utilisé un OCR très efficace via un logiciel acheté, [PDF Element](https://pdf.wondershare.fr/). Je l'ai trouvé meilleur que le moteur OCR [tesseract](https://github.com/tesseract-ocr/tesseract). Cependant, je n'ai pas poussé les possibilités du moteur jusqu'au bout. *tesseract* a été implémenté dans R dans un [package](https://docs.ropensci.org/tesseract/) [@R-tesseract]. Si vous avez un pdf et que la reconnaissance de texte est bonne, vous pouvez utiliser le package [pdftools](https://docs.ropensci.org/pdftools/) [@R-pdftools] pour extraire le texte et le manipuler avec R.   

Une fois que le texte de votre corpus est prêt à être importé dans R, vous pouvez commencer l'analyse. Julia Silge et David Robinson [-@robinson2020] ont écrit un excellent [livre d'introduction](https://www.tidytextmining.com/index.html) au text mining dans R. Il s'appuie sur le package [tidytext](https://github.com/juliasilge/tidytext) [@R-tidytext;@tidytext2016], qui apporte des fonctions simples pour faire des choses compliquées (tokeniser des textes, calculer des statistiques comme [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf), *etc.*). Si vous voulez en savoir plus sur la tokenisation, le stemming et la lemmatisation, les stop words, ou les embeddings de mots, vous pouvez passer à l'étape suivante et lire la première partie de [Supervised Machine Learning for Text Analysis in R](https://smltar.com/) [@silge2021]. 

Une fois que vous avez acquis une certaine maîtrise de l'extraction de texte, de son nettoyage et de son prétraitement, et du calcul de statistiques standard, vous pouvez vous intéresser à des techniques plus complexes et à l'apprentissage automatique. Dans ce cas, la modélisation thématique peut offrir des résultats très riches. Il s'agit de l'utilisation d'un modèle (le Latent Dirichlet Allocation étant le plus utilisé) pour identifier *k* sujets (*k* étant déterminé par le modélisateur) dans votre corpus. Le résultat de votre modélisation thématique est que chaque document de votre corpus est représenté comme un mélange de thèmes, et chaque thème comme un mélange de mots. Pour exécuter la modélisation thématique dans R, vous trouverez des informations intéressantes dans le [chapitre](https://www.tidytextmining.com/topicmodeling.html) correspondant de Silge et Robinson [-@robinson2020]. Dans ce chapitre, ils utilisent le package *topicmodels* [@R-topicmodels;@topicmodels2011].^[Voir le [blog](https://juliasilge.com/blog/) de Julia Silge pour d'autres exemples de modélisation de sujets en R--et pour de nombreux autres exemples de modélisation et de prédiction avec R]. Mais je trouve que le package [stm](https://cran.r-project.org/web/packages/stm/vignettes/stmVignette.pdf) [@R-stm] est aussi très utile avec de très belles fonctions implémentées. Il vous permet de croiser votre modélisation thématique avec les métadonnées de votre corpus. 

Enfin, pour vous aider à faire fonctionner la modélisation des sujets dans R (mais aussi pour d'autres problématiques de text mining), vous pouvez consulter les conseils et bonnes pratiques discutés par @welbers2017a ou @banks2018. Plus généralement, R offre de nombreuses ressources pour le text mining et le Natural Language Processing (NLP) : vous pouvez trouver un aperçu des différents packages existants [ici](https://www.bnosac.be/index.php/blog/87-an-overview-of-the-nlp-ecosystem-in-r-nlproc-textasdata).

```{r, echo = FALSE, fig.align='center', fig.cap="L'écosystème du Natural Language Processing avec R"}
knitr::include_graphics('NLP-R-ecosystem.png')
```


## Se plonger dans la modélisation et l'apprentissage automatique

Je ne suis pas du tout un spécialiste de la modélisation et de l'apprentissage automatique, mais j'espère avoir l'occasion de me former dans les années à venir. En effet, il y a de nombreuses occasions où cela peut être utile pour les historiens. Si vous voulez vous rafraîchir la mémoire en économétrie en apprenant R, vous pouvez regarder le livre de Schmelzer [-@schmelzer2021] que vous pouvez lire [en ligne](https://www.econometrics-with-r.org/). En ce qui concerne l'apprentissage automatique, qui peut avoir de nombreuses applications en histoire de l'économie, notamment avec les techniques de classification, j'ai trouvé que @greenwell2020 offre une bonne introduction à l'implémentation en R de différents algorithmes d'apprentissage automatique - vous pouvez le lire [ici](https://bradleyboehmke.github.io/HOML/).

Pour aider les utilisateurs de R dans leur travail de modélisation, la collection de packages [tidymodels](https://www.tidymodels.org/) peut être très utile, que ce soit pour le prétraitement de vos données, l'évaluation de votre modèle ou l'ajustement de vos paramètres. Le site web contient également de nombreux tutoriels intéressants pour apprendre l'approche *tidymodels*.

## La programmation plus loin : Fonctions, fonctions, fonctions... et packages

Une fois que vous avez progressé dans la maîtrise de ces différentes méthodes et que vous avez amélioré vos compétences en matière de codage R, une tâche importante consiste à solidifier votre code et à le rendre fiable pour les travaux futurs. La première étape lorsque vous écrivez du code est de le documenter : donnez à votre script une structure claire, expliquez pourquoi vous utilisez cette fonction, gardez une trace des chemins qui ont échoué, indiquez ce que vous pourriez améliorer, *etc*. Cela peut être utile pour les collaborateurs ou les lecteurs de vos scripts, mais surtout pour vous, car un script désordonné est le meilleur moyen de perdre beaucoup de temps deux ou trois mois plus tard, lorsque vous devrez revenir sur votre projet (par exemple après avoir reçu les rapports des arbitres sur votre article). Vous pouvez trouver des conseils utiles [ici](https://style.tidyverse.org/index.html) sur la structuration de votre script et le nommage des objets. Comme l'explique @wickham2017 [ici](https://r4ds.had.co.nz/program-intro.html), il est également très utile de revenir sur votre script et de le réécrire pour le rendre plus simple et plus clair.

Une étape importante de cette réécriture consiste à transformer une partie de votre code en fonctions. Une bonne règle générale est que si vous copiez et collez la même partie de votre code plus de deux fois, il pourrait être utile de la transformer en fonction. Vous trouverez des informations utiles sur la programmation correcte des fonctions dans @wickham2019, à commencer par le [chapitre sur les fonctions](https://r4ds.had.co.nz/functions.html)^[Si vous programmez avec *dplyr*, cette [vignette](https://dplyr.tidyverse.org/articles/programming.html#introduction-1) peut également vous être utile. Idem pour *data.table* [ici](https://www.r-bloggers.com/2020/01/programming-with-data-table/).].


Si vous avez un ensemble de fonctions que vous utilisez régulièrement dans un ou plusieurs de vos projets, vous devriez envisager d'écrire un package. Même si vous ne prévoyez pas initialement de partager votre code, l'écriture d'un *package* est une bonne discipline pour écrire du bon code et pour la reproductibilité. Plus tard, il sera utile de faire circuler votre code et, peut-être, si vous êtes satisfait du résultat et pensez qu'il pourrait être utile à d'autres codeurs R, vous voudrez le publier sur le [CRAN](https://cran.r-project.org/). Pour l'écriture d'un package, @wickham2021a propose un guide essentiel que vous pouvez également trouver en ligne [ici](https://r-pkgs.org/index.html). Si vous souhaitez partager votre *package*, [pkgdown](https://pkgdown.r-lib.org/) vous aide à créer facilement un site web documentant votre *package*.


## Rédiger et diffuser votre travail avec R Mardown


Enfin, vous aurez probablement besoin de diffuser et de publier vos résultats. Heureusement, [R Markdown](https://rmarkdown.rstudio.com/) est l'outil idéal pour cela. Outre la possibilité d'exécuter du code dans votre document R Markdown et donc d'afficher les résultats, il offre de nombreux avantages comme la gestion bibliographique et l'intégration de [Zotero](https://www.zotero.org/)^[Vous pouvez lire [ceci](https://rstudio.github.io/visual-markdown-editing/citations.html) pour voir comment Rstudio facilite l'interaction entre *R Mardown* et Zotero]. Il permet de créer des documents *html*, *pdf* et *word*. Pour *pdf*, il s'appuie sur [LaTeX](https://www.latex-project.org/) et utilise [Pandoc](https://pandoc.org/) pour la conversion à partir du [langage markdown](https://en.wikipedia.org/wiki/Markdown).^[Voir ce [billet de blog](https://programminghistorian.org/en/lessons/sustainable-authorship-in-plain-text-using-pandoc-and-markdown) pour connaître les avantages de l'écriture en *markdown* et de l'utilisation de *pandoc* plutôt que d'un éditeur de texte comme *Word*]. Avec *R Markdown*, vous pourrez rédiger des rapports (automatisés) et des [tableaux de bord](https://pkgs.rstudio.com/flexdashboard/index.html), des articles universitaires, des livres et des thèses [@R-bookdown ; @bookdown2016], des diapositives, et même créer votre propre site Web [@R-blogdown ; @blogdown2017] et des applications [@R-shiny ; @wickham2021b]. 

Pour apprendre à utiliser *R Markdown*, vous pouvez vous référer à @rmarkdown2018 et @riederer2021 avec les sites web correspondants [ici](https://bookdown.org/yihui/rmarkdown/) et [ici](https://bookdown.org/yihui/rmarkdown-cookbook/).

{{% callout note %}} Vous pouvez trouver d'autres tutoriels grâce à la très grande liste du [Big Book of R](https://www.bigbookofr.com/index.html). {{% /callout %}}

## Bibliography
