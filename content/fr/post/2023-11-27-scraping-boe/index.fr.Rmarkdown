---
title: "Moissonner des documents du site de la Banque d'Angleterre"
author: "Aurélien Goutsmedt"
date: "2023-11-28"
slug: "scraping-boe"
categories:
- Scraping
- R Tutorials
- Central Banks
tags:
- Scraping
- R
- Research Tools
- Quantitative Analysis
- Central Banks
subtitle: "Scraping Tutorials 3"
summary: "Dans ce billet, vous apprendrez à récupérer divers documents de recherche sur le site de la [Banque d'Angleterre](https://www.bankofengland.co.uk/)."
authors: []
lastmod: "`r Sys.Date()`"
featured: no
draft: no
lang: fr
bibliography: bibliography.bib
link-citations: yes
color-links: yes
linkcolor: teal
urlcolor: blue
citecolor: red
endnote: no
toc: yes
number_sections: true
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

{{% toc %}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, fig.dim=c(14,10), fig.align = "center")
```

Dans le [premier tutoriel](/fr/post/scraping-bis) sur le web scraping, j'ai expliqué comment récupérer la [base de données des discours de banquiers centraux de la Banque des règlements internationaux](https://www.bis.org/cbspeeches/index.htm?m=256). Le [deuxième tutoriel](/fr/post/scraping-ecb) s'est concentré sur les documents de travail du site web de la [Banque centrale européenne](https://www.ecb.europa.eu/home/html/index.en.html). Dans ce troisième billet, je montrerai une méthode pour récupérer des documents sur le site de la [Banque d'Angleterre](https://www.bankofengland.co.uk/). 

Dans les deux premiers billets, nous avions une liste de documents sur une page web avec différentes informations (ou "métadonnées") que nous voulions récupérer. Pour la BRI, nous avons dû comprendre la structure de l'URL, l'utiliser pour changer de page et récupérer les métadonnées de tous les discours qui nous intéressaient. Pour la BCE, nous devions plutôt faire défiler la page vers le bas et cliquer sur des menus pour ouvrir des détails supplémentaires. Ici, nous allons utiliser une approche radicalement différente, consistant à extraire la structure du site web, le "sitemap", pour voir où nous pouvons trouver les métadonnées des documents.^[Cette méthode est inspirée d'un script initialement écrit par [François Claveau](https://www.usherbrooke.ca/recherche/specialistes/details/francois.claveau) et [Jérémie Dion](https://www.epistemopratique.org/chaire/equipe/dion-jeremie/) pour un [projet de recherche sur la Banque d'Angleterre](https://www.rebuildingmacroeconomics.ac.uk/academia-policy-pipeline).]

Contrairement aux deux premiers posts, nous n'aurons pas besoin de [RSelenium](https://docs.ropensci.org/RSelenium/) [@R-rselenium] ici. Nous n'utiliserons que [rvest](https://rvest.tidyverse.org/) [@R-rvest] et [polite](https://dmi3kno.github.io/polite/) [@R-polite]. En effet, la méthode choisie n'implique pas d'interaction avec un navigateur web. 

```{r needed-packages}
# pacman is a useful package to install missing packages and load them
if(! "pacman" %in% installed.packages()){
  install.packages("pacman", dependencies = TRUE)
}

pacman::p_load(tidyverse,
               rvest,
               polite)
```

## Explorer le site de la BoE

Jetons un coup d'oeil, par exemple, aux discours de la BoE sur `https://www.bankofengland.co.uk/news/speeches`.

```{r show-op, fig.cap="Les discours de la BoE",  echo=FALSE}
knitr::include_graphics("featured.png")
```

Nous avons une liste de discours relativement similaire à celle que nous avions pour la BRI, avec une liste de pages que vous pouvez changer au bas de la page web. Cependant, l'URL reste la même lorsque vous passez à une autre page. Je n'ai pas trouvé de moyen de manipuler l'URL pour passer d'une page à l'autre. Mais vous pouvez facilement utiliser *RSelenium* pour cliquer sur le bouton afin de passer à la page suivante. 

Néanmoins, nous pouvons collecter plus d'informations en cliquant sur chaque discours, car chaque discours a une page web dédiée comme celle-ci : 

```{r, fig.cap="La page web d'un discours",  echo=FALSE}
knitr::include_graphics("example_speech.png")
```

Cela signifie que pour chaque discours des responsables politiques de la BoE, il existe une URL, et que nous devrions donc être en mesure de trouver cette URL à partir du "sitemap".^[En fait, la même méthode pourrait être utilisée pour récupérer les discours de la [base de données de la Banque des règlements internationaux](https://www.bis.org/cbspeeches/index.htm?m=256).] 

## Explorer la structure du site

Comme expliqué dans le [premier tutoriel](/fr/post/scraping-bis), il est important de déclarer qui nous sommes au site web et de vérifier les règles de scraping sur ce site web en lisant le *robot.txt*.  

```{r bowing}
root_url_BoE <- "https://www.bankofengland.co.uk"
user_agent <- "polite R package - used for https://github.com/agoutsmedt/central_bank_database (aurelien.goutsmedt[at]uclouvain.be)"
session <- bow(root_url_BoE,
               user_agent = user_agent)

session$robotstxt$text
```

Le *robot.txt* nous indique également l'URL du *sitemap* :

```{r sitemap}
sitemap <- session$robotstxt$sitemap$value

sitemap
```

Cette page web rassemble une (très longue) liste de tous les URLs existants du site web. Vous pouvez essayer d'y jeter un coup d'oeil dans votre navigateur, mais le temps de chargement est très long. Le plus simple est de la lire directement ici via *rvest*.

```{r xml_sitemap}
xml_sitemap <- rvest::read_html(sitemap)

xml_sitemap %>% html_element(xpath = "body")
```

On voit que c'est l'argument "loc" qui nous intéresse.

```{r url-list}
data_frame_sitemap <- tibble(hyperlink= xml_sitemap %>% 
                                   rvest::html_elements(xpath = ".//loc") %>% 
                                   rvest::html_text()) %>% 
  filter(hyperlink != root_url_BoE) # we exclude the base url that we don't need anymore

data_frame_sitemap
```

Nous avons une liste de `r nrow(data_frame_sitemap)` URLs. Ce qu'il nous faut maintenant, c'est comprendre le contenu de ces URLs : nous pouvons le savoir en regardant ce qui suit l'URL de base :

```{r categories}
data_frame_sitemap <- data_frame_sitemap %>% 
  mutate(category = str_remove(hyperlink, "https://www.bankofengland.co.uk/") %>% 
                     str_remove("/.*"))

unique(data_frame_sitemap$category) 
```

Nous pouvons maintenant filtrer les URL en fonction de ce que nous voulons récupérer. Pour cette fois, oublions les discours et disons que nous sommes intéressés par les "minutes" :

```{r}
minutes_urls <- data_frame_sitemap %>% 
  filter(category == "minutes")

minutes_urls
```

## Récupérer les métadonnées des *Minutes*

Grâce aux URL des *minutes*, nous avons les dates des *minutes.* Concentrons-nous sur les *minutes* de 2023.

```{r}
minutes_urls <- minutes_urls %>% 
  mutate(dates = str_extract(hyperlink, "(?<=/)\\d{4}(?=/)") %>% as.integer()) %>% 
  filter(dates == 2023)
```

Nous pouvons jeter un coup d'œil au premier URL de nos données :

```{r, fig.cap="Un exemple de page de minutes",  echo=FALSE}
knitr::include_graphics("example_minute.png")
```

En utilisant *rvest*, nous extrayons d'abord le code `.html` de la première page web. Vous pouvez voir que le texte du procès-verbal peut être extrait directement de cette page web. Nous voulons extraire le titre du document, sa date, son résumé et le contenu du texte.

```{r}
page <- minutes_urls %>% 
  slice(1) %>% 
  pull(hyperlink) %>% 
  read_html

extracted_title <-  page %>% 
  html_nodes("h1") %>% 
  html_text()
extracted_date <- page %>%  
  html_nodes(".published-date") %>% 
  html_text(trim = TRUE) %>% 
  str_extract("\\d+ [A-z]+ \\d+$") %>% 
  dmy()

# As the abstract and the text may gather several paragraphs, the object could be a vector of several strings. We thus save these strings as a list format (because we want one line per URL/document)
abstract <-  page %>% 
  html_nodes(".hero") %>% 
  html_text(trim= TRUE) %>% 
  list()
text <- page %>% 
  # We extract the titles and the text together
  html_nodes('.page-section .content-block h2 , .page-section .content-block h3, .page-section .content-block .page-content p')  %>% 
  html_text() %>% 
  list()

metadata <- tibble(title = extracted_title,
                   date = extracted_date,
                   abstract = abstract,
                   text = text)  

metadata
```

Nous pouvons jeter un coup d'œil aux premières lignes du texte extrait :

```{r}
metadata %>%
  pull(text) %>% 
  unlist() %>% 
  .[1:30] %>%  
  str_c(collapse = "\n") %>% 
  cat()
```

Il ne nous reste plus qu'à construire une boucle pour récupérer les métadonnées de toutes les minutes. Pour éviter de surcharger le site web du BoE, nous devons fixer un délai entre la navigation sur les différentes URL. 

```{r, include=FALSE}
delay <- 1
```

```{r, eval=FALSE}
delay <- session$delay
```

Nous utiliserons également la fonction `TryCatch()` pour éviter de rompre la boucle en cas d'erreur (par exemple lorsqu'une page n'est pas chargée correctement).

```{r results='hide'}
errors <- list()
metadata <- vector(mode = "list", length = nrow(minutes_urls))

  for(i in minutes_urls$hyperlink){ 
    tryCatch({
      page <- read_html(i)
      cat(paste("Now scraping URL: ", i, "\n"))
      Sys.sleep(delay)
      
      extracted_title <-  page %>% 
        html_nodes("h1") %>% 
        html_text()
      extracted_date <- page %>%  
        html_nodes(".published-date") %>% 
        html_text(trim = TRUE) %>% 
        str_extract("\\d+ [A-z]+ \\d+$") %>% 
        dmy()
      abstract <-  page %>% 
        html_nodes(".hero") %>% 
        html_text(trim= TRUE) %>% 
        list()
      text <- page %>% 
  # We extract the titles and the text together
      html_nodes('.page-section .content-block h2 , .page-section .content-block h3, .page-section .content-block .page-content p')  %>% 
      html_text() %>% 
        list()
      
      metadata[[i]] <- tibble(title = extracted_title,
                              url = i,
                              date = extracted_date,
                              abstract = abstract,
                              text = text)   
    }, error = function(e) {
      cat(paste("Problem with scraping URL: ", i, "\n"))
    errors[[i]] <- i
    })
  }

minutes_metadata <- bind_rows(metadata)

minutes_metadata
```

Lorsque tous les documents ont leur propre page web, les données *sitemap* deviennent une information puissante pour faciliter votre scraping. Ici, elle peut être utilisée pour récupérer facilement tous les documents publiés par la Banque d'Angleterre. Cependant, la structure de la page n'est pas exactement la même au sein d'une catégorie (le texte des minutes n'est pas disponible sur les pages web avant 2022 mais vous pouvez récupérer l'URL du PDF), et entre les catégories (regardez par exemple un discours ou un document de travail). Par conséquent, un code plus raffiné est nécessaire. Vous trouverez un tel script [ici](https://github.com/agoutsmedt/central_bank_database/blob/master/scraping_scripts/scraping_boe.R).


## Bibliographie {-}

