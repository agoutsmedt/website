---
title: "Moissonner les discours des banquiers centraux à partir du site de la BRI"
author: "Aurélien Goutsmedt"
date: "2023-11-09"
slug: "scraping-bis"
categories:
- Scraping
- R Tutorials
- Central Banks
tags:
- Scraping
- R
- Research Tools
- Quantitative Analysis
- Central Banks
subtitle: "Scraping Tutorials 1"
summary: "Dans ce billet, vous apprendrez comment moissonner les discours de la [base de données de la Banque des règlements internationaux](https://www.bis.org/cbspeeches/index.htm?m=256) qui rassemble la plupart des discours des banquiers centraux en anglais."
authors: []
lastmod: "`r Sys.Date()`"
featured: no
draft: no
lang: en
bibliography: bibliography.bib
suppress-bibliography: no
link-citations: yes
color-links: yes
linkcolor: teal
urlcolor: blue
citecolor: red
endnote: no
toc: yes
number_sections: true
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

{{% toc %}}


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, warning = FALSE, message = FALSE, fig.dim=c(14,10), fig.align = "center")
```

## Moissonner des données

Dans ce billet, vous apprendrez comment récupérer les discours de la [base de données des discours de la Banque des règlements internationaux] (https://www.bis.org/cbspeeches/index.htm?m=256), qui rassemble la plupart des discours des banquiers centraux en anglais. Nous le ferons en programmant en R. Le [deuxième tutoriel](/fr/post/scraping-ecb) se concentre sur l'extraction de documents du site web de la [Banque centrale européenne](https://www.ecb.europa.eu/home/html/index.en.html) et le [troisième tutoriel](/fr/post/scraping-boe) sur l'extraction de documents du site web de la Banque d'Angleterre.

### Qu'est-ce que le *web scraping* ?

Le *web scraping* est le processus d'extraction de données à partir de sites web. Il s'agit d'accéder de manière programmatique au contenu d'un site web, d'en extraire les données souhaitées et de les sauvegarder en vue d'une utilisation ultérieure. Il s'agit d'accéder à une URL spécifique, de récupérer le contenu html et d'extraire les données souhaitées à partir de ce contenu.

En général, on utilise le *web scraping* lorsqu'il n'existe pas d'[API](https://fr.wikipedia.org/wiki/Interface_de_programmation) (Application Programming Interface) pour les données du site web auxquelles vous souhaitez accéder. Une API est un moyen structuré et autorisé d'accéder aux données fournies par un site web, à partir d'un système de requêtes (par exemple, [scopus](https://www.scopus.com/) fournit une API pour [récupérer ses données bibliométriques](../extracting-biblio-data-1/)). 

L'extraction de données d'un site web soulève diverses questions éthiques et juridiques (pour lesquelles je ne suis pas un spécialiste !). Tout d'abord, de nombreux sites web ont des conditions d'utilisation qui interdisent explicitement le *web scraping*. S'engager dans le scraping sans le consentement du propriétaire du site web constitue une violation de ces termes et conditions. Par exemple, [Linkedin](https://www.linkedin.com/) interdit le scraping sur son site web. Toutefois, la "légalité" du scraping sur Linkedin [reste une question complexe](https://www.forbes.com/sites/zacharysmith/2022/04/18/scraping-data-from-linkedin-profiles-is-legal-appeals-court-rules/). Avant de vous lancer dans le moissonnage, vous devez connaître les conditions d'utilisation et les risques encourus si vous les enfreignez.

Un deuxième problème concerne la surcharge des serveurs. Le *web scraping* peut faire peser une charge importante sur les serveurs d'un site web, ce qui peut entraîner une baisse des performances ou des temps d'arrêt. Le moissonnage sans limitation de débit appropriée et sans respect de la capacité d'un site web peut être considéré comme contraire à l'éthique et perturbateur.

Troisièmement, si vous êtes autorisé à explorer un site web, cela ne signifie pas que vous pouvez copier et distribuer le matériel collecté. Vous devez être conscient des droits d'auteur et de propriété intellectuelle lorsque vous diffusez du contenu moissonné.

### Quelques ressources utiles

Lorsque l'on parle de *web scraping* avec R, trois *packages* R se distinguent (mais nous n'en utiliserons que deux ici) :

- [rvest](https://rvest.tidyverse.org/) [@R-rvest] : un package dans l'esprit [tidyverse](https://www.tidyverse.org/) qui permet d'extraire des données de pages web. Il fonctionne mieux avec les pages web statiques (voir ci-dessous). 
- RSelenium](https://docs.ropensci.org/RSelenium/) [@R-rselenium] : il s'agit d'un *binding R* pour *[Selenium](https://www.selenium.dev/) 2.0 Remote WebDriver*, qui permet d'automatiser l'utilisation d'un navigateur. En d'autres termes, il vous permet d'écrire un script pour ouvrir un navigateur web et interagir avec un site web (et c'est là que le scraping commence à devenir amusant !).
- polite](https://dmi3kno.github.io/polite/) [@R-polite] : ce *package* vous permet de consulter le [robots.txt](https://en.wikipedia.org/wiki/Robots.txt) d'un site web pour savoir quelles sont les règles de *scraping* pour ce site web, et pour éviter de surcharger le site avec trop de requêtes.

En fonction du site web que vous souhaitez moissonner, l'utilisation de [rvest](https://rvest.tidyverse.org/) peut être suffisant (comme c'est le cas, par exemple, avec [Wikipedia](https://en.wikipedia.org/wiki/Main_Page)). Pour les sites web plus complexes, vous aurez peut-être besoin de [RSelenium](https://docs.ropensci.org/RSelenium/), en combinaison avec [rvest](https://rvest.tidyverse.org/) ou seul. En effet, tout dépend si les pages web que vous souhaitez récupérer sont statiques ou dynamiques. Comme l'explique [l'article de blog d'Ivan Milanes](https://ivanmillanes.netlify.app/post/2020-06-30-webscraping-with-rselenium-and-rvest/) :

- [Page Web statique](https://www.pcmag.com/encyclopedia/term/static-web-page) : Une page web (page HTML) qui contient les mêmes informations pour tous les utilisateurs. Bien qu'elle puisse être mise à jour périodiquement, elle ne change pas à chaque fois que l'utilisateur la consulte.
- [Page Web dynamique](https://www.pcmag.com/encyclopedia/term/dynamic-web-page) : Une page Web qui fournit un contenu personnalisé à l'utilisateur en fonction des résultats d'une recherche ou d'une autre requête. Également appelé "HTML dynamique" ou "contenu dynamique", le terme "dynamique" est utilisé pour désigner les pages Web interactives créées pour chaque utilisateur.


La page web où sont rassemblés les discours des banquiers centraux est une page web dynamique et nous utiliserons donc *RSelenium* pour extraire les discours.^[Je ne suis pas un spécialiste des structures html, mais je n'ai pas réussi à extraire les données en utilisant uniquement *rvest*. J'ai donc utilisé *RSelenium*, et *RSelenium* uniquement, même si la plupart du temps on combine les deux packages].


Pour compléter ce tutoriel, vous trouverez deux autres tutoriels utiles sur le scraping à l'aide de *rvest* et *RSelenium* [ici](https://www.rselenium-teaching.etiennebacher.com/#/title-slide) et [ici](https://resulumit.com/teaching/scrp_workshop). Il existe également un [chapitre entier consacré au scraping web](https://r4ds.hadley.nz/webscraping) dans la récente deuxième édition de *R for Data Science* [@wickham2023].


## Moissonner les discours de la BRI

Si nous nous intéressons à ce que disent les banquiers centraux, nous pouvons vouloir étudier leurs discours. Heureusement, la plupart d'entre eux sont stockés sur le site web de la BRI [là]("https://www.bis.org/cbspeeches/index.htm"). 

```{r bis-website, fig.cap="La liste des discours de la BRI", echo=FALSE}
knitr::include_graphics("bis.png")
```

Ce que nous voulons, c'est :

- extraire les métadonnées de ces discours : la date du discours, le titre, la description et l'orateur. 
- d'accéder au texte complet du discours, qui est stocké dans un PDF sur le site web de la BIS
- d'extraire le texte de ce PDF
- s'assurer que le texte est lisible, ce qui sera le cas si le PDF est "cherchable" (c'est-à-dire si les caractères sont reconnus). 

C'est tout cela que nous allons apprendre à faire dans ce tutoriel.

### Chargement des *packages* et démarrage du navigateur automatisé

Nous commençons par charger les *packages* nécessaires. [RSelenium](https://docs.ropensci.org/RSelenium/) [@R-rselenium] et [polite](https://dmi3kno.github.io/polite/) sont les paquets centraux ici. J'aime bien le *package* [here](https://here.r-lib.org/) pour manipuler les chemins avec R mais il n'est pas essentiel ici. [glue](https://glue.tidyverse.org/) est utilisé pour créer des chaînes de caractères plus facilement, en y intégrant des expressions R. [pdftools](https://docs.ropensci.org/pdftools/) sera utilisé pour extraire le texte des PDF des discours et [tesseract](https://docs.ropensci.org/tesseract/) pour lancer la reconnaissance optique de caractères (OCR) sur les PDFs qui ne disposent pas d'un bon OCR.

```{r needed-packages}
# pacman is a useful package to install missing packages and load them
if(! "pacman" %in% installed.packages()){
  install.packages("pacman", dependencies = TRUE)
}

pacman::p_load(tidyverse,
               RSelenium,
               polite,
               glue,
               here,
               pdftools,
               tesseract)

bis_data_path <- here(path.expand("~"), "data", "central_bank_database", "BIS") # Where data will go on my computer
```

### Aller sur le site web de la BRI et lancer le *bot*

Nous utilisons *polite* pour nous "incliner" devant le site web de la BRI. Cela nous permet d'examiner les autorisations pour le *scraping*.

```{r bowing-bis}
bis_website_path <- "https://www.bis.org/"

session <- bow(bis_website_path)
session
```

En examinant le fichier *robots.txt*, nous pouvons voir plus en détail quels chemins du site web sont moissonnables ou non. 

```{r robotstxt}
cat(session$robotstxt$text)
```

La liste des discours se trouve à ["https://www.bis.org/cbspeeches/index.htm"](https://www.bis.org/cbspeeches/index.htm). Ce chemin ne fait pas partie de la liste "disallow", ce qui signifie que nous pouvons récupérer cette partie du site web.

Afin de faciliter le *scraping*, nous afficherons le nombre maximal d'éléments par page, qui est ici de 25. Nous ne voulons pas moissonner toute la liste des discours, nous allons donc fixer une date de départ : le 9 octobre 2023. En naviguant vers la deuxième page de la liste obtenue, vous pouvez maintenant avoir une idée de ce à quoi ressemble l'URL.

```{r bis-filtered, fig.cap="Discours filtrés sur le site de la BRI",  echo=FALSE}
knitr::include_graphics("bis_filtered.png")
```

Ainsi, vous pouvez préparer une URL pré-formatée, dans laquelle vous pouvez modifier les paramètres. 

```{r scraping_path}
scraping_path <- expr(glue("{bis_website_path}cbspeeches/index.htm?fromDate={day}%2F{month}%2F{year}&cbspeeches_page={page}&cbspeeches_page_length=25"))
```

Ici, j'aurais pu utiliser un code plus simple en utilisant la fonction `paste0()` par exemple, plutôt que d'utiliser `expr()`. Mais je veux pouvoir changer les paramètres plus tard, notamment pour le numéro de page. Pour l'instant, le chemin ressemble à ceci :

```{r example-expr}
scraping_path
```

Mais je peux alors entrer les paramètres que je souhaite pour la date (ici, le 9 octobre 2023).

```{r entering-date}
day <- "09"
month <- "10"
year <- 2023
```

Et je peux évaluer l'expression avec ces paramètres, en entrant également une certaine page. Cela sera utile plus tard dans une boucle pour naviguer d'une page à l'autre.

```{r example-eval}
eval(scraping_path, envir = list(page = 3))
```

Vous pouvez vérifier que cette URL vous conduit à la troisième page de la liste des discours commençant le 9 octobre 2023 (avec 25 résultats par page).

Une fois que vous avez la structure de l'URL, cela signifie que vous pouvez modifier les paramètres à votre guise pour créer la liste de discours que vous souhaitez. Bien sûr, vous pouvez aller sur le site de la BRI et filtrer par la date de fin ou par les "institutions" ou le "pays" pour voir à quoi ressemblerait l'URL.

Maintenant, nous pouvons commencer la partie amusante. Lançons notre navigateur automatisé avec *RSelenium* :


```{r launch-browser, results="hide"}
# Launch Selenium to go on the website of bis
driver <- rsDriver(browser = "firefox", # can also be "chrome"
                   chromever = NULL,
                   port = 4444L) 
remote_driver <- driver[["client"]]
```

```{r bot-picture, fig.cap="Automatizing your web browser",  echo=FALSE}
knitr::include_graphics("featured.png")
```

### Extraction des données page par page

Ce que nous voulons faire, c'est extraire les données relatives à chaque discours (la date, le titre, la description, l'orateur et l'URL correspondante) page par page. Nous pouvons faire cela avec une boucle `for`. Mais nous devons d'abord connaître le nombre de pages. Vous pouvez normalement le voir en bas à droite de la page web. 

Il existe différentes façons d'extraire cette information. Ici, nous utiliserons le "sélecteur CSS". L'utilisation du sélecteur CSS présente deux avantages :

- son contenu est généralement plus simple que celui d'un XPATH
- Il peut être facilement récupéré grâce à un module complémentaire de Firefox (qui existe également pour Chrome) : [ScrapeMate](https://addons.mozilla.org/fr/firefox/addon/scrapemate/)

Une fois l'add-on installé, vous pouvez aller sur l'URL sur laquelle nous travaillons et trouver le "CSS selector" pour le nombre de pages :

```{r scrape-mate, fig.cap="Using scrapemate Firefox addon",  echo=FALSE}
knitr::include_graphics("scrapmate.png")
```

Vous pouvez utiliser ce sélecteur CSS pour extraire le texte grâce à *RSelenium* et après un court nettoyage, vous obtenez le nombre total de pages :

```{r nb-pages}
remote_driver$navigate(eval(scraping_path, envir = list(page = 1)))
Sys.sleep(2) # Just to have time to load the page

nb_pages <- remote_driver$findElement("css selector", ".pageof")$getElementText() %>% # We just set i for searching the total number of pages
  pluck(1) %>% 
  str_remove_all("Page 1 of ") %>%
  as.integer()

nb_pages
```

Ok. Maintenant, nous pouvons exécuter la boucle pour récupérer toutes les données des `r nb_pages` pages. Ici, nous nous conformerons au délai fixé par le paquet *polite* : nous attendrons `r session$delay` secondes pour chaque page, afin de ne pas surcharger le site web avec trop de requêtes. Comme nous avons besoin que la page web se charge correctement pour récupérer les données, c'est aussi un moyen de prendre le temps de charger la page pour éviter de manquer des données si votre connexion internet est trop lente.

Il suffit d'utiliser *scrapemate* pour trouver le sélecteur CSS approprié pour chaque information et extraire le texte correspondant. Vous pouvez également voir qu'en cliquant sur le titre du discours, vous naviguez vers la page web dédiée au discours correspondant. Cela signifie qu'au titre est associé un lien URL, un "href" en langage html. Il faut donc extraire l'attribut "href" associé au titre grâce à *RSelenium*.

```{r extract-metadata}
metadata <- vector(mode = "list", length = nb_pages)

for(page in 1:nb_pages){
  remote_driver$navigate(eval(scraping_path))
  nod <- nod(session, eval(scraping_path)) # introducing to the new page
  Sys.sleep(session$delay) # using the delay time set by polite

  metadata[[page]] <- tibble(date = remote_driver$findElements("css selector", ".item_date") %>% 
                            map_chr(., ~.$getElementText()[[1]]),
                          info = remote_driver$findElements("css selector", ".item_date+ td") %>% 
                            map_chr(., ~.$getElementText()[[1]]),
                          url = remote_driver$findElements("css selector", ".dark") %>% 
                            map_chr(., ~.$getElementAttribute("href")[[1]])) 
}

metadata <- bind_rows(metadata) %>% 
  separate(info, c("title", "description", "speaker"), "\n")

head(metadata)
```

La fonction `findElements()` de *RSelenium* trouve tous les éléments correspondants (ici 25) et les stocke dans une liste. La fonction `map_chr()` permet d'obtenir le texte de chaque élément un par un et stocke le résultat dans un vecteur. 

Si nous voulons être sûrs d'avoir récupéré les données de tous les discours, nous pouvons comparer la longueur de notre tableau de données avec le nombre d'éléments indiqué juste au-dessus du premier discours.

```{r check-items}
# We just check that we have the correct number of speeches
nb_items <- remote_driver$findElement("css selector", ".listitems")$getElementText() %>% # We just set i for searching the total number of pages
  pluck(1) %>%
  str_remove_all("[\\sitems]") %>% 
  as.integer()
if(nb_items == nrow(metadata)){
  cat("You have extracted the metadata of all speeches")
} else{
  cat("There is a problem. You have probably missed some speeches.")
}
```

### Télécharger les PDFs

Nous disposons à présent des métadonnées des discours. Mais ce qui nous intéresse à la fin, c'est de savoir de quoi parlent les discours. La plupart des discours sont stockés dans un fichier PDF. Pour récupérer ces PDF, il suffit d'utiliser l'URL du discours, et de remplacer la fin de l'URL ".htm" par ".pdf". À partir de la liste des métadonnées, on peut obtenir la liste complète des PDF que l'on souhaite télécharger.

```{r list-pdf}
pdf_to_download <- metadata %>% 
    mutate(pdf_name = str_extract(url, "r\\d{6}[a-z]"),
           pdf_name = str_c(pdf_name, ".pdf")) %>% 
    pull(pdf_name) 

pdf_to_download
```

Nous construisons une fonction pour télécharger ces PDF en utilisant *polite*. Cela évitera notamment de surcharger le site.

```{r polite-download}
polite_download <- function(domain, url, ...){
  bow(domain) %>% 
    nod(glue(url)) %>% 
    rip(...)  
}
```

Il est possible que certains PDF soient manquants (ce n'est pas le cas ici, mais cela arrive avec l'ensemble des discours de la BRI). Le problème est que si un PDF est manquant, vous obtiendrez une erreur et le processus de téléchargement du PDF sera interrompu. Vous devez donc utiliser la fonction `tryCatch()` pour prendre en compte cette erreur mais continuer le processus de téléchargement.

```{r download-pdf, eval=FALSE}
tryCatch(
   {
     walk(pdf_to_download, ~polite_download(domain = bis_website_path,
                                            url = glue("review/{.}"),
                                            path = here(bis_data_path, "pdf"),))
   },
   error = function(e) {
     print(glue("No pdf exist"))
   }
  )
```

Ici, le pdf a été stocké dans un dépôt spécifique sur mon ordinateur, indiqué par le paramètre `path`.

## Extraire le texte des discours

Maintenant que nous avons collecté les PDF, nous voulons extraire le texte contenu dans ces PDF. Nous utiliserons pour cela le paquet [pdftools](https://docs.ropensci.org/pdftools/). Voici une fonction qui prend la liste des PDFs extraits des métadonnées, extrait le texte, et le stocke dans un tableau avec le nom du fichier et la page correspondante du texte (en effet, `pdf_text()` extrait le texte page par page). Ici, nous enregistrons l'erreur au cas où nous n'aurions pas trouvé le PDF : nous ne voulons pas que l'extraction du texte s'arrête, mais nous voulons nous souvenir qu'un PDF est manquant.

```{r extracting-function}
extracting_text <- function(file, path) {
  tryCatch(
    {  
  message(glue("Extracting text from {file}"))
  text_data <- pdf_text(here(path, file)) %>%
    as_tibble() %>% 
    mutate(page = 1:n(),
           file = file) %>% 
    rename(text = value)
    },
  error = function(e) {
    print(glue("No pdf exist"))
    text_data <- tibble(text = NA,
                        page = 1,
                        file = file)
  }
  )
}
```

Nous pouvons exécuter la fonction pour chaque discours que nous avons dans notre ensemble de données. 

```{r extract-text}
text_data <- map(pdf_to_download, ~extracting_text(., here(bis_data_path, "pdf"))) %>% 
  bind_rows()

head(text_data)
```

### Gestion des problèmes d'OCR

Il arrive que les PDF présentent des problèmes d'OCR. Les PDF ne sont pas "recherchables" et le texte ne peut donc pas être extrait correctement (ou plutôt, il est illisible). Cela se produit également avec des discours récents. Nous devons donc trouver les PDFs susceptibles de rencontrer de tels problèmes. Nous comptons le nombre de mots dans chaque page et récupérons les PDFs pour lesquels le nombre de mots est faible dans chaque page. 

```{r ocr-problem}
pdf_without_ocr <- text_data %>% 
  mutate(test_ocr = str_count(text, "[a-z]+")) %>% # Count the number of words
  mutate(non_ocr_document = all(test_ocr < 250), .by = file) %>% 
  filter(non_ocr_document == TRUE) %>% 
  distinct(file) %>% 
  pull()

pdf_without_ocr
```

Le fait de fixer le seuil à 250 mots est une simple règle empirique dans ce cas précis : cela permet de n'avoir que des vrais positifs, mais n'empêche pas d'avoir des faux négatifs (des documents avec un problème d'OCR que nous avons raté). Il devrait être possible de trouver une meilleure méthode pour évaluer quels PDFs n'ont pas d'OCR. 

Voici un exemple de PDF problématique :

```{r look-ocr-problem}
text_data %>% 
  filter(file %in% pdf_without_ocr[1])
```


Il semble correct lorsqu'on le regarde, mais en allant sur la page web on voit qu'il y a des problèmes de sélection dans le texte et que l'OCR n'est donc pas adéquate.

```{r show-ocr-problem, fig.cap="PDF sans OCR adéquate",  echo=FALSE}
knitr::include_graphics("problem_ocr.png")
```

Nous allons maintenant utiliser le paquet [tesseract](https://docs.ropensci.org/tesseract/), qui exécute l'OCR. Il convertit d'abord chaque page du PDF en une image (PNG), puis procède à l'OCR et extrait le texte.

```{r tesseract, eval=FALSE}
new_text_ocr <- vector(mode = "list", length = length(pdf_without_ocr))

for(i in 1:length(pdf_without_ocr)){
  text <- tesseract::ocr(here(bis_data_path, "pdf", pdf_without_ocr[i]), engine = tesseract::tesseract("eng"))
  text <- tibble(text) %>% 
    mutate(text = str_remove(text, ".+(=?\\\n)"),
           page = 1:n(),
           file = str_remove(pdf_without_ocr[i], "\\.pdf"))
  new_text_ocr[[i]] <- text
}

new_text_ocr <- bind_rows(new_text_ocr)
```

```{r tesseract_hidden, eval=TRUE, echo=FALSE}
for(i in 1){
  text <- tesseract::ocr(here(bis_data_path, "pdf", pdf_without_ocr[i]), engine = tesseract::tesseract("eng"))
  text <- tibble(text) %>% 
    mutate(text = str_remove(text, ".+(=?\\\n)"),
           page = 1:n(),
           file = str_remove(pdf_without_ocr[i], "\\.pdf"))
  new_text_ocr <- text
}
```

Vous pouvez constater que le texte est maintenant lisible :

```{r show-correction}
new_text_ocr
```

Il ne vous reste plus qu'à intégrer le texte reconnu au reste des données. Une dernière petite étape consiste à supprimer tous les PNGs que vous avez créés en utilisant *tesseract*.

```{r removing-pictures, results="hide"}
# We remove all the .png we have created
png_to_remove <- list.files()[str_which(list.files(), "_\\d+\\.png")]
file.remove(png_to_remove)
```

Si vous souhaitez récupérer les discours de la BRI sur une base régulière sans récupérer à nouveau ce que vous avez déjà collecté, vous pouvez jeter un oeil à ce script [ici](https://github.com/agoutsmedt/central_bank_database/blob/master/scraping_scripts/scraping_bis.R), qui fait partie d'un [projet de collecte de documents des banques centrales](https://github.com/agoutsmedt/central_bank_database/tree/master). Vous y trouverez également des fonctions pour nettoyer les données de la BRI.


## Bibliography {-}

