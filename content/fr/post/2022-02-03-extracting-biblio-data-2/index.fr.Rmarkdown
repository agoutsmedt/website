---
title: "Extracting and Cleaning Bibliometric Data with R (2)"
author: Aur√©lien Goutsmedt
date: '2022-02-03'
slug: extracting-biblio-data-2
categories:
  - Bibliometrics
  - Network Analysis
  - R Tutorials
tags:
  - Bibliometrics
  - Network Analysis
  - Quantitative Analysis
  - R
  - Research Tools
subtitle: 'Exploring Dimensions'
summary: 'In this post, you will learn how to extract data from Dimensions website and how to clean them. These data allow you to build bibliographic networks.'
authors: []
lastmod: '2022-02-03T22:19:06+01:00'
featured: no
draft: no
lang: en
bibliography: bibliography.bib
#csl:  chicago-author-date.csl
suppress-bibliography: false
link-citations: true
color-links: true # See https://ctan.org/pkg/xcolor for colors
linkcolor: teal
urlcolor: blue
citecolor: red
endnote: false
toc: true
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
---

{{% toc %}}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

The last [post](/post/extracting-biblio-data-1) dealt with extracting bibliometric data from Scopus and presented some steps to clean these data, notably references data, with R. We will do something similar here, but for another database: [Dimensions](https://dimensions.ai). Dimensions is a relatively newcomer in the world of bibliometric database, in comparison to Scopus or Web of Science. The initial advantage of Dimensions is that the [search application](https://app.dimensions.ai/discover/publication) is open to any one (if I remember well, you just have to create an account).^[It exists also an [API](https://docs.dimensions.ai/dsl/) for which you can ask access. It would allow you to use the [rdimensions package](https://github.com/nicholasmfraser/rdimensions) to query data.]

## Search on Dimensions website

So let's begin with the same search than in the last post: publications using Dynamic Stochastic General Equilibrium model.^[Keeping the same search allows us to do some comparisons with Scopus data and observe how it impacts networks and other results.] Like with Scopus, we search for "DSGE" *or* "Dynamic Stochastic General Equilibrium". A difference here is that we can choose between search in "full data" (meaning searching also in the full-text) or in "Title and abstract" (see Figure \@ref(fig:dimensions-search)). Doing the first search, we get 27 693 results on February 4, 2022. The fact that we get close to 100 results before the 1990s (when the label DSGE began to be used) means that we could have a lot of false positive. It should be explored more in-depth. But for now and for this tutorial, we will opt for a more secure search, by focusing on titles and abstracts. 

```{r dimensions-search, fig.cap="Our search on Dimensions app", echo=FALSE}
knitr::include_graphics("dimensions_search.png")
```

We obtain 4106 results on February 4, 2022. That is quite much than the 2633 results of our Scopus search in the last [post](/post/extracting-biblio-data-1#using-scopus-website). But I think that it can be partly explain by the fact that Dimensions integrate "preprints" and we have 1532 preprints in our results. 

To download the result, click on "Save / Export" on the right of the search bar, and then "Export results". Two types of export are of interest for us: "Export full record" and "Export for bibliometric mapping". In the first case, we can export only 500 results against 2500 for the second one (see Figure \@ref(fig:dimensions-export)). This is not obvious, but the second option allows you to export most metadata, including affiliations and the references cited. What do we lose in comparison to the first export? The publication type (article, preprints, etc...), acknowledgements [that can be useful; see for instance @paul-hus2017] and categorization of publications. That is not so much. Besides, it means that, by using the second option, you can extract more results than in Scopus (limited to 2000). 

```{r dimensions-export, fig.cap="Choosing the export format", echo=FALSE}
knitr::include_graphics("dimensions_export.png")
```

Another advantage of Dimensions over Scopus is that the data exported are quite easier to clean. So let's load the needed packages and start the cleaning.

```{r}
# packages
package_list <- c(
  "here", # use for paths creation
  "tidyverse",
  "janitor", # useful functions for cleaning imported data
  "biblionetwork", # creating edges
  "tidygraph", # for creating networks
  "ggraph" # plotting networks
)
for (p in package_list) {
  if (p %in% installed.packages() == FALSE) {
    install.packages(p, dependencies = TRUE)
  }
  library(p, character.only = TRUE)
}

github_list <- c(
  "agoutsmedt/networkflow", # manipulating network
  "ParkerICI/vite" # needed for the spatialisation of the network
)
for (p in github_list) {
  if (gsub(".*/", "", p) %in% installed.packages() == FALSE) {
    devtools::install_github(p)
  }
  library(gsub(".*/", "", p), character.only = TRUE)
}

# paths
data_path <- here(path.expand("~"),
                  "data",
                  "tuto_biblio_dsge")
dimensions_path <- here(data_path, 
                    "dimensions")
```

## Cleaning Dimensions data

Here are the data, extracted in two steps as we have more than 2500 items:

```{r loading-data}
dimensions_data_1 <- read_csv(here(dimensions_path,
                                "dimensions_dsge_data_2015-2022.csv"),
                           skip = 1)
dimensions_data_2 <- read_csv(here(dimensions_path,
                                            "dimensions_dsge_data_1996-2014.csv"),
                                       skip = 1)

dimensions_data <- rbind(dimensions_data_1,
                                  dimensions_data_2) %>% 
  clean_names() # cleaning column names with janitor to transform them

dimensions_data
```

We don't need to give an ID to our documents as there is already a `publications_id` column.^[A good practice is to clean and standardise column names by using the great [`janitor` package](http://sfirke.github.io/janitor/). By default, `clean_names()` use [snake case names](https://en.wikipedia.org/wiki/Snake_case).] 

### Cleaning duplicates 

One issue with Dimensions is that it integrates preprints, which can be interesting for certain types of analysis.^[For instance, by studying what is eventually published and after how much time.] However, by using the "Export for bibliometric mapping" option (see the [previous section](#search-on-dimensions-website)), we cannot know if an item is a preprint or not. The consequence is that *i)* we can have duplicates (the same article with the preprint(s) and then the published version) but *ii)* we cannot discriminate between these duplicates as we don't know which one is the published version. What we can do is to spot the duplicates thanks to the title and keep one of the duplicates depending on the presence of references or not.^[It remains the possibility of "hidden" duplicates if the title have changed when the document has been published.] In other words, we keep the document for which we have references, and we do not care of which one we keep if all have references.^[Obviously, that is not the optimal method if what you want is to keep the published version. Here, you would need to exclude the preprints of the duplicates, for instance by removing items which display "SSRN" as their `source_title_anthology_title`. Another option, if you don't want preprints at all is just to uncheck preprints on Dimensions search application when you are extracting your data.] 

```{r}
duplicated_articles <- dimensions_data %>%
  add_count(title) %>% 
  filter(n > 1) %>% 
  arrange(title, cited_references)

to_keep <- duplicated_articles %>% 
  group_by(title) %>% 
  slice_head(n = 1)

to_remove <- duplicated_articles %>% 
  filter(! publication_id %in% to_keep$publication_id)

dimensions_data <- dimensions_data %>% 
  filter(! publication_id %in% to_remove$publication_id)
```

### Cleaning affiliations

The goal now is to create a table which associates each document to authors' affiliation (column `authors_affiliations_name_of_research_organization`) and the country of the affiliation (column `authors_affiliations_country_of_research_organization`). We cannot associate affiliations directly to authors as documents can have more authors than affiliations, and conversely.

```{r}
dimensions_affiliations <- dimensions_data %>% 
  select(publication_id,
         authors_affiliations_name_of_research_organization,
         authors_affiliations_country_of_research_organization) %>% 
  separate_rows(starts_with("authors"), sep = "; ")

knitr::kable(head(dimensions_affiliations))
```

We can now compare the country of affiliations for the dimensions document and for the Scopus affiliation cleaned in the [previous post](/post/extracting-biblio-data-1#extracting-affiliations-and-authors). We first bind together affiliations of DSGE articles from Scopus and Dimensions.

```{r binding-affiliations}
scopus_affiliations <- read_rds(here(data_path, 
                                     "scopus", 
                                     "data_cleaned", 
                                     "scopus_affiliations.rds")) %>% 
  mutate(data = "scopus") %>% 
  select(-authors)

dimensions_affiliations <- dimensions_affiliations %>% 
  mutate(data = "dimensions")

affiliations <- dimensions_affiliations %>% 
  rename("affiliations" = authors_affiliations_name_of_research_organization,
         "country" = authors_affiliations_country_of_research_organization,
         "citing_id" = publication_id) %>% 
  bind_rows(scopus_affiliations)
```

In Figure \@ref(fig:country_affiliations), we can observe that the ranking is almost the same at least for the first 8 countries. Nonetheless, while we have more documents in Dimensions(`r nrow(dimensions_data)`) than in Scopus (`r length(unique(scopus_affiliations$citing_id))`), we have a relatively equivalent number of affiliations in both corpus: `r nrow(dimensions_affiliations)` for Dimensions against `r nrow(scopus_affiliations)` for Scopus. Besides, we count more `NA` values (`r nrow(filter(affiliations, data == "dimensions" & is.na(country)))`) than affiliations from United States (`r nrow(filter(affiliations, data == "dimensions" & country == "United States"))`) in Dimensions, meaning that a lot of information is missing.

```{r country-affiliations, fig.cap="Top countries for affiliations in Scopus and Dimensions", fig.dim=c(12,9)}
affiliations %>% 
  filter(!is.na(country)) %>% 
  group_by(data) %>% 
  count(country) %>% 
  mutate(proportion = n/sum(n)) %>% 
  slice_max(order_by = proportion, n = 10, with_ties = FALSE) %>% 
  mutate(country = tidytext::reorder_within(country, proportion, data)) %>% # see https://juliasilge.com/blog/reorder-within/ for more info
  ggplot(aes(proportion, country, fill = data)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~data, scales = "free_y") + 
  tidytext::scale_y_reordered() +
  labs(x = "Proportion of the Country in all affiliations (excluding NA)") +
  theme_classic(base_size = 16)
```

### Cleaning references

Cleaning the references is quite easy for Dimensions:

- you have a clear separator between the references of each citing document: a semi-colon before a square bracket;
- you have the same information for each references (see `column_names`) even if the information is lacking, and you have a simple separator between information: `|`.

```{r cleaning-ref}
references_extract <- dimensions_data %>% 
  filter(! is.na(cited_references)) %>% 
  rename("citing_id" = publication_id) %>% # because the "publication_id" column is also the identifier of the reference
  select(citing_id, cited_references) %>% 
  separate_rows(cited_references, sep = ";(?=\\[)") %>%
  as_tibble

column_names <- c("authors",
                  "author_id",
                  "source",
                  "year",
                  "volume",
                  "issue",
                  "pagination",
                  "doi",
                  "publication_id",
                  "times_cited")

dimensions_direct_citation <- references_extract %>% 
  separate(col = cited_references, into = column_names, sep = "\\|")

knitr::kable(head(dimensions_direct_citation, n = 5))
```

Another advantage is that, as for [Scopus API](/post/extracting-biblio-data-1#alternative-method-using-scopus-apis-and-rscopus) (but not for data extracted on Scopus website), references already have an identifier.^[Be careful: the column is also called `publication_id`, meaning that you have to change the name of the column with the identifier of citing articles.] It means that you don't have to match the citations together to find which documents are citing the same references (what we had to do [here](/post/extracting-biblio-data-1#matching-references)). Thus, you can easily extract the list of the distinct references cited in your corpus, and it will make easier the building of a co-citation network later.

```{r extracting_ref}
dimensions_references <- dimensions_direct_citation %>% 
  distinct(publication_id, .keep_all = TRUE) %>% 
  select(-citing_id)

knitr::kable(tail(dimensions_references, n = 5))
```

However, there is one problem with Dimensions ID: I suppose that Dimensions is able to match references together thanks to DOI that are often present in references data (more than in Scopus references data). The problem is that some references (in general books and chapters in books) lack many (actually almost all) information, like authors and publishing source. It means that we are not able to know what it is, if not by entering the DOI in a web browser. We will encounter this problem again when building the [network](#building-co-citation-network). 

As for affiliations country, we can compare the most cited references in Dimensions and in Scopus. We first extract the top 10 for Scopus and Dimensions:

```{r top-ref}
scopus_direct_citation <- read_rds(here(data_path,
                                        "scopus",
                                        "data_cleaned",
                                        "scopus_manual_direct_citation.rds"))
scopus_references <- read_rds(here(data_path,
                                   "scopus",
                                   "data_cleaned",
                                   "scopus_manual_references.rds"))

scopus_top_ref <- scopus_direct_citation %>% 
  add_count(new_id_ref) %>% 
  mutate(proportion = n/n()) %>% 
  select(new_id_ref, proportion) %>% 
  unique() %>% 
  slice_max(proportion, n = 10) %>%
  left_join(select(scopus_references, new_id_ref, references)) %>% 
  select(references, proportion) %>% 
  mutate(data = "scopus",
         references = str_extract(references, ".*\\(\\d{4}\\)")) # cleaning for plotting

dimensions_references <- dimensions_references %>% 
  mutate(references = paste0(authors, ", ", year, ", ", source))

dimensions_top_ref <- dimensions_direct_citation %>% 
  add_count(publication_id) %>% 
  mutate(proportion = n/n()) %>% 
  select(publication_id, proportion) %>% 
  unique() %>% 
  slice_max(proportion, n = 10) %>%
  left_join(select(dimensions_references, publication_id, references)) %>% 
  select(references, proportion) %>% 
  mutate(data = "dimensions")
```

We can see in Figure \@ref(fig:top-ref-plot) that the 10 most cited references are the same, only the ranking is changing after the fourth position

```{r top-ref-plot, fig.cap="Top references of DSGE documents in Scopus and Dimensions", fig.dim=c(12,9)}
dimensions_top_ref %>% 
  bind_rows(scopus_top_ref) %>% 
  mutate(references = str_wrap(references, 35)) %>% 
  ggplot(aes(proportion, fct_reorder(references, proportion), fill = data)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~data, scales = "free") + 
  tidytext::scale_y_reordered() +
  labs(x = "Proportion on all citations") +
  theme_classic(base_size = 13)
```

## Building co-citation network

Here, we follow what we have done for Scopus [here](/post/extracting-biblio-data-1#bibliographic-co-citation-analysis). We use the `biblio_cocitation` function of the [`biblionetwork`](https://agoutsmedt.github.io/biblionetwork/) package to build the list of edges between our nodes (i.e. the references). The edge between two nodes is weighted depending of the total number of times each reference has been cited in the whole Dimensions DSGE corpus (see [here](https://agoutsmedt.github.io/biblionetwork/reference/biblio_cocitation.html) for more details).

```{r creating-edges}
citations <- dimensions_direct_citation %>% 
  add_count(publication_id) %>% 
  select(publication_id, n) %>% 
  unique

references_filtered <- dimensions_references %>% 
  left_join(citations) %>% 
  filter(n >= 5)

edges <- biblionetwork::biblio_cocitation(filter(dimensions_direct_citation, publication_id %in% references_filtered$publication_id), 
                                          "citing_id", 
                                          "publication_id",
                                          weight_threshold = 3)
edges
```

We can now create a graph using [`tidygraph`](https://tidygraph.data-imaginist.com/index.html) [@R-tidygraph] and [`networkflow`](https://github.com/agoutsmedt/networkflow).^[I don't enter in the details here as that is not the purpose of this tutorial and that the different steps are explained on `networkflow` [website](https://github.com/agoutsmedt/networkflow).]

```{r building-graph}
references_filtered <- references_filtered %>% 
  relocate(publication_id, .before = authors) # first column has to be the identifier

graph <- tbl_main_component(nodes = references_filtered, 
                            edges = edges, 
                            directed = FALSE)
graph
```

```{r manipulating-graph}
set.seed(1234)
graph <- leiden_workflow(graph) # identifying clusters of nodes 

nb_communities <- graph %>% 
  activate(nodes) %>% 
  as_tibble %>% 
  select(Com_ID) %>% 
  unique %>% 
  nrow
palette <- scico::scico(n = nb_communities, palette = "hawaii") %>% # creating a color palette
    sample()
  
graph <- community_colors(graph, palette, community_column = "Com_ID")

graph <- graph %>% 
  activate(nodes) %>%
  mutate(size = n,# will be used for size of nodes
         first_author = str_extract(authors, "(?<=\\[)(.+?)(?=,)"),
         label = paste0(first_author, "-", year)) 

graph <- community_names(graph, 
                         ordering_column = "size", 
                         naming = "label", 
                         community_column = "Com_ID")

graph <- vite::complete_forceatlas2(graph, 
                                    first.iter = 10000)


top_nodes  <- top_nodes(graph, 
                        ordering_column = "size", 
                        top_n = 15, 
                        top_n_per_com = 2,
                        biggest_community = TRUE,
                        community_threshold = 0.02)
community_labels <- community_labels(graph, 
                                     community_name_column = "Community_name",
                                     community_size_column = "Size_com",
                                     biggest_community = TRUE,
                                     community_threshold = 0.02)
```

A co-citation network allows us to observe what are the main influences of a field of research. At the centre of the network, we find the most cited references. On the borders of the graph, there are specific communities that influence different parts of the literature on DSGE. Here, the size of nodes depends on the number of times they are cited in our corpus.

If we compare to the [Scopus co-citation network](/post/extracting-biblio-data-1#bibliographic-co-citation-analysis), we find some similarities as the domination of the @smets2007a community.^[However, we can observe that if this community is still the biggest, it is bigger than in Scopus network and include more central references.] However, the network is less dense here and displays more marginal communities. The "NA-1994" community is a good example of what I was talking about above on [Dimensions identifying process](#cleaning-references). "NA-1994" (the biggest node of this community) is actually @hamilton1994.

```{r projecting-graph, eval=FALSE}
graph <- graph %>% 
  activate(edges) %>% 
  filter(weight > 0.05)

ggraph(graph, "manual", x = x, y = y) + 
  geom_edge_arc0(aes(color = color_edges, width = weight), alpha = 0.4, strength = 0.2, show.legend = FALSE) +
  scale_edge_width_continuous(range = c(0.1,2)) +
  scale_edge_colour_identity() +
  geom_node_point(aes(x=x, y=y, size = size, fill = color), pch = 21, alpha = 0.7, show.legend = FALSE) +
  scale_size_continuous(range = c(0.3,16)) +
  scale_fill_identity() +
  ggnewscale::new_scale("size") +
  ggrepel::geom_text_repel(data = top_nodes, aes(x=x, y=y, label = Label), size = 2, fontface="bold", alpha = 1, point.padding=NA, show.legend = FALSE) +
  ggrepel::geom_label_repel(data = community_labels, aes(x=x, y=y, label = Community_name, fill = color), size = 6, fontface="bold", alpha = 0.9, point.padding=NA, show.legend = FALSE) +
  scale_size_continuous(range = c(0.5,5)) +
  theme_void()
```

```{r plotting-network, echo = FALSE}
ragg::agg_png(here("content", "en", "post", "2022-02-03-extracting-biblio-data-2", "featured.png"),
              width = 30, 
              height = 25,
              units = "cm",
              res = 200)
ggraph(graph, "manual", x = x, y = y) + 
  geom_edge_arc0(aes(color = color_edges, width = weight), alpha = 0.4, strength = 0.2, show.legend = FALSE) +
  scale_edge_width_continuous(range = c(0.1,2)) +
  scale_edge_colour_identity() +
  geom_node_point(aes(x=x, y=y, size = size, fill = color), pch = 21, alpha = 0.7, show.legend = FALSE) +
  scale_size_continuous(range = c(0.3,16)) +
  scale_fill_identity() +
  ggnewscale::new_scale("size") +
  ggrepel::geom_text_repel(data = top_nodes, aes(x=x, y=y, label = Label), size = 2, fontface="bold", alpha = 1, point.padding=NA, show.legend = FALSE) +
  ggrepel::geom_label_repel(data = community_labels, aes(x=x, y=y, label = Community_name, fill = color), size = 6, fontface="bold", alpha = 0.9, point.padding=NA, show.legend = FALSE) +
  scale_size_continuous(range = c(0.5,5)) +
  ggdark::dark_theme_void()
invisible(dev.off())
```

```{r display-network, echo=FALSE, fig.cap="Bibliographic coupling network of articles using DSGE models"}
knitr::include_graphics("featured.png")
```

The differences between Scopus and Dimensions co-citation network would deserve a more thorough investigation, which is outside the scope of this tutorial.^[Part of the differences comes from differences in the initial corpus and thus is linked to the difference in Scopus and Dimensions data (notably the journals that are integrated in the databases). Another part of the differences results from our own treatment of the data: a different cleaning of references, different thresholds used to select the nodes (as we use an absolute threshold, not a relative one), different types of documents (preprints and "hidden" duplicates), etc...] So let's conclude just by observing what are the most cited nodes in each community. Here, we can see more clearly that the community `02` represent to a certain extent the "core" of the literature at the basis of DSGE models.

```{r top-ref-community, fig.cap="Most cited references per communities", fig.dim=c(12,10)}
top_nodes(graph,
          ordering_column = "size",
          top_n = 10,
          top_n_per_com = 6,
          biggest_community = TRUE,
          community_threshold = 0.03) %>% 
  select(Community_name, references, n) %>% 
  mutate(label = str_wrap(references, 35),
         label = tidytext::reorder_within(label, n, Community_name)) %>% 
  ggplot(aes(n, label, fill = Community_name)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~Community_name, ncol = 3, scales = "free") +
  tidytext::scale_y_reordered() +
  labs(x = "Number of citations", y = NULL) +
  theme_classic(base_size = 10)
```

# References
